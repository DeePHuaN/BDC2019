{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68528D4BD2FF477E835367876AA05991"
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录\n",
    "!ls /home/kesci/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78C2D92E19964A869FD2BB072B7D2DD7"
   },
   "outputs": [],
   "source": [
    "# 显示cell运行时长\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EB679DDC9A3E4506BBD8B7AC814269D1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3F7495B9CB16475882586C73C8BE107C"
   },
   "outputs": [],
   "source": [
    "mypath = \"/home/kesci/work/drift_data/\"\n",
    "path = \"/home/kesci/input/bytedance/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73FB4DA7D9AB49F38B68B645C240A4B6",
    "mdEditEnable": false
   },
   "source": [
    "### 拿出前1E数据和最后1E数据，单独存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9536583C033472C861A43567562DA82"
   },
   "outputs": [],
   "source": [
    "# 取出前1E数据\n",
    "train = pd.read_csv(\"/home/kesci/input/bytedance/train_final.csv\",header=None,names=[\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"],\n",
    "                    nrows=100000000)\n",
    "train.to_csv(mypath+\"train_first_100million.csv\",index=False,header=None)\n",
    "# 取出最后1E数据\n",
    "train = pd.read_csv(\"/home/kesci/input/bytedance/train_final.csv\",header=None,names=[\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"],\n",
    "                    skiprows = 900000000)\n",
    "train.to_csv(\"/home/kesci/work/train_last_100million.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A18780D07EE64EC088CB38487E89A0F9",
    "mdEditEnable": false
   },
   "source": [
    "# 一、word2vec训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8F2FE61CB744726A2C0F036B2834B21",
    "mdEditEnable": false
   },
   "source": [
    "### 训练前1E数据word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2E61EAF5FB2D4DE88F64CFD1544FCADB"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(mypath+\"train_first_100million.csv\",header=None,names=[\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"],\n",
    "                    chunksize=10000000)\n",
    "test = pd.read_csv(path+\"test_final_part1.csv\",header=None,names=[\"query_id\",\"query\",\"query_title_id\",\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9D9EFB70FEB44C71868AD6F655CB015C"
   },
   "outputs": [],
   "source": [
    "#训练train\n",
    "class Word2VecTrainingMaster():\n",
    "    def __init__(self, work_path=mypath, base_name='drift_word2vec.model', embed_size=300, min_word_count=2,\n",
    "                 window=5, auto=True, step_save=False):\n",
    "\n",
    "        self.work_path = work_path\n",
    "        self.base_name = base_name\n",
    "        self.word2vec_model = self.make_word2vec_model(embed_size, min_word_count, window)\n",
    "        self.word2vec_model.save(self.work_path + '/' + self.base_name)\n",
    "        self.step_save = step_save\n",
    "\n",
    "        if auto:\n",
    "            INIT = True\n",
    "            for chunk in train:\n",
    "                print(chunk.shape)\n",
    "                corpus = list(chunk[\"title\"].apply(lambda row : row.split())) + list(chunk[\"query\"].apply(lambda row : row.split()))\n",
    "                print(corpus[-1])\n",
    "                self.update_model(corpus, init=INIT)\n",
    "                INIT = False\n",
    "                self.word2vec_model.save(self.work_path + '/' + self.base_name)\n",
    "                print(len(self.word2vec_model.wv.vocab))\n",
    "\n",
    "    def update_model(self, batch, init):\n",
    "        if init:\n",
    "            self.word2vec_model.build_vocab(batch)\n",
    "        else:\n",
    "            self.word2vec_model.build_vocab(batch, update=True)\n",
    "        self.word2vec_model.train(batch, total_examples=self.word2vec_model.corpus_count,\n",
    "                                  epochs=self.word2vec_model.iter)\n",
    "        if self.step_save:\n",
    "            self.word2vec_model.save(self.work_path + '/' + self.base_name)\n",
    "\n",
    "    def make_word2vec_model(self, embed_size, min_word_count, window):\n",
    "        return Word2Vec(size=embed_size, min_count=min_word_count, window=window, workers=100, seed=2019, iter=10)\n",
    "\n",
    "w2v = Word2VecTrainingMaster(embed_size=300)\n",
    "print(len(w2v.word2vec_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "042AA21C541D42B9877D6DFEACC4CD14",
    "mdEditEnable": false
   },
   "source": [
    "### 测试集1 2000W训练word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6F545F67C8E41DF80ECDD0A6559E362"
   },
   "outputs": [],
   "source": [
    "class Word2VecTrainingMaster():\n",
    "    def __init__(self, work_path=mypath, base_name='drift_word2vec.model', embed_size=300, min_word_count=2,\n",
    "                 window=5, auto=True, step_save=False):\n",
    "\n",
    "        self.work_path = work_path\n",
    "        self.base_name = base_name\n",
    "        self.word2vec_model = self.make_word2vec_model(embed_size, min_word_count, window)\n",
    "        #self.word2vec_model.save(self.work_path + '/' + self.base_name)\n",
    "        self.step_save = step_save\n",
    "        \n",
    "        print(len(self.word2vec_model.wv.vocab))\n",
    "        \n",
    "        if auto:\n",
    "            INIT = False\n",
    "            corpus = list(test[\"title\"].apply(lambda row : row.split())) + list(test[\"query\"].apply(lambda row : row.split()))\n",
    "            self.update_model(corpus, init=INIT)\n",
    "            self.word2vec_model.save(self.work_path + '/' + self.base_name)\n",
    "            print(len(self.word2vec_model.wv.vocab))\n",
    "\n",
    "    def update_model(self, batch, init):\n",
    "        if init:\n",
    "            self.word2vec_model.build_vocab(batch)\n",
    "        else:\n",
    "            self.word2vec_model.build_vocab(batch, update=True)\n",
    "        self.word2vec_model.train(batch, total_examples=self.word2vec_model.corpus_count,\n",
    "                                  epochs=self.word2vec_model.iter)\n",
    "        if self.step_save:\n",
    "            self.word2vec_model.save(self.work_path + '/' + self.base_name)\n",
    "\n",
    "    def make_word2vec_model(self, embed_size, min_word_count, window):\n",
    "        #return Word2Vec(size=embed_size, min_count=min_word_count, window=window, workers=4, seed=2019, iter=5)\n",
    "        return Word2Vec.load(\"/home/kesci/work/drift_data/drift_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4B66FCE877454F66816CEF4EA148F94F"
   },
   "outputs": [],
   "source": [
    "model = Word2VecTrainingMaster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "319170AD31614DD282BDFA2E98ADC4E0",
    "mdEditEnable": false
   },
   "source": [
    "### 最后一亿数据训练word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1759AD6FE0E346FEA210BD626ECA9875"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/kesci/work/train_last_100million.csv\",chunksize=20000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BC781022F8FB4127827F52C349098C79"
   },
   "outputs": [],
   "source": [
    "#训练train\n",
    "class Word2VecTrainingMaster():\n",
    "    def __init__(self, work_path=mypath, base_name='drift_word2vec_v99.model', embed_size=300, min_word_count=5,\n",
    "                 window=5, auto=True, step_save=False):\n",
    "\n",
    "        self.work_path = work_path\n",
    "        self.base_name = base_name\n",
    "        self.word2vec_model = self.make_word2vec_model(embed_size, min_word_count, window)\n",
    "        #self.word2vec_model.save(self.work_path + '/' + self.base_name)\n",
    "        self.step_save = step_save\n",
    "\n",
    "        if auto:\n",
    "            INIT = False\n",
    "            for chunk in train:\n",
    "                print(chunk.shape)\n",
    "                corpus = list(chunk[\"title\"].apply(lambda row : row.split())) + list(chunk[\"query\"].apply(lambda row : row.split()))\n",
    "                print(corpus[0])\n",
    "                self.update_model(corpus, init=INIT)\n",
    "                self.word2vec_model.save(self.work_path + '/' + self.base_name)\n",
    "                print(len(self.word2vec_model.wv.vocab))\n",
    "\n",
    "    def update_model(self, batch, init):\n",
    "        if init:\n",
    "            self.word2vec_model.build_vocab(batch)\n",
    "        else:\n",
    "            self.word2vec_model.build_vocab(batch, update=True)\n",
    "        self.word2vec_model.train(batch, total_examples=self.word2vec_model.corpus_count,\n",
    "                                  epochs=self.word2vec_model.iter)\n",
    "        if self.step_save:\n",
    "            self.word2vec_model.save(self.work_path + '/' + self.base_name)\n",
    "\n",
    "    def make_word2vec_model(self, embed_size, min_word_count, window):\n",
    "        #return Word2Vec(size=embed_size, min_count=min_word_count, window=window, workers=100, seed=2019, iter=10)\n",
    "        return Word2Vec.load(\"/home/kesci/work/drift_data/drift_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4104E97639634E8687CEB35D686C01CA"
   },
   "outputs": [],
   "source": [
    "w2v = Word2VecTrainingMaster(embed_size=300)\n",
    "print(len(w2v.word2vec_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87DDA9D967EC4C1384AD1FB3D7B09179",
    "mdEditEnable": false
   },
   "source": [
    "### 测试集2 1E训练word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3A0B7FEAFD044209B50D80BBC9E4D3CB"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\",names=[\"query_id\",\"query\",\"query_title_id\",\n",
    "                                        \"title\"],chunksize=20000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87F0DC815946411E856881A567D89C24"
   },
   "outputs": [],
   "source": [
    "#训练train\n",
    "class Word2VecTrainingMaster():\n",
    "    def __init__(self, work_path=mypath, base_name='drift_word2vec_v100.model', embed_size=300, min_word_count=4,\n",
    "                 window=5, auto=True, step_save=False):\n",
    "\n",
    "        self.work_path = work_path\n",
    "        self.base_name = base_name\n",
    "        self.word2vec_model = self.make_word2vec_model(embed_size, min_word_count, window)\n",
    "        #self.word2vec_model.save(self.work_path + '/' + self.base_name)\n",
    "        self.step_save = step_save\n",
    "\n",
    "        if auto:\n",
    "            INIT = False\n",
    "            for chunk in test:\n",
    "                print(chunk.shape)\n",
    "                corpus = list(chunk[\"title\"].apply(lambda row : row.strip().split())) + list(chunk[\"query\"].apply(lambda row : row.strip().split()))\n",
    "                print(corpus[0])\n",
    "                self.update_model(corpus, init=INIT)\n",
    "                self.word2vec_model.save(self.work_path + '/' + self.base_name)\n",
    "                print(len(self.word2vec_model.wv.vocab))\n",
    "\n",
    "    def update_model(self, batch, init):\n",
    "        if init:\n",
    "            self.word2vec_model.build_vocab(batch)\n",
    "        else:\n",
    "            self.word2vec_model.build_vocab(batch, update=True)\n",
    "        self.word2vec_model.train(batch, total_examples=self.word2vec_model.corpus_count,\n",
    "                                  epochs=self.word2vec_model.iter)\n",
    "        if self.step_save:\n",
    "            self.word2vec_model.save(self.work_path + '/' + self.base_name)\n",
    "\n",
    "    def make_word2vec_model(self, embed_size, min_word_count, window):\n",
    "        #return Word2Vec(size=embed_size, min_count=min_word_count, window=window, workers=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CDC7A70CAC540D8829263852DCE730F"
   },
   "outputs": [],
   "source": [
    "w2v = Word2VecTrainingMaster(embed_size=300)\n",
    "print(len(w2v.word2vec_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9D3962BC561A42BF89504804DF397307"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "649E0F2658BA412992254E0A5B9C310B",
    "mdEditEnable": false
   },
   "source": [
    "# 二、生成词典映射Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5F5EEEC4D74419C8ED97C26C36F43E2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import fastText \n",
    "import keras \n",
    "\n",
    "from tqdm import tqdm\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CBAEC0272C24D6F8ED2DAC2F445AA5B"
   },
   "outputs": [],
   "source": [
    "path = \"/home/kesci/input/bytedance/\"\n",
    "mypath = \"/home/kesci/work/drift_data/\"\n",
    "# train_path = mypath+\"train_first_100million.csv\"\n",
    "test_path = mypath+\"test_final_part1.csv\"\n",
    "\n",
    "batch_size = 1024\n",
    "num_words = 900000\n",
    "embed_size = 300\n",
    "query_max_sequence_length = 10\n",
    "title_max_sequence_length = 22\n",
    "max_sequence_length = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "853A297743AB4824BDC63960007BB1BA"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/kesci/work/train_last_100million.csv\")#,nrows=20000000)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1191CD0FB0DD41ADA24EFDA3AF35A001"
   },
   "outputs": [],
   "source": [
    "train_query = train[\"query\"].drop_duplicates()\n",
    "train_title = train[\"title\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66CDB90C197247DB822BCA2728AFBB7A"
   },
   "outputs": [],
   "source": [
    "test_stage1 = pd.read_csv(\"/home/kesci/input/bytedance/test_final_part1.csv\",header=None,names=[\"query_id\",\"query\",\"query_title_id\",\"title\"])\n",
    "test_stage1[\"title\"] = test_stage1[\"title\"].apply(lambda row : row.strip())\n",
    "print(test_stage1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DCCFA78A7354AD4914A9886713C5DDD"
   },
   "outputs": [],
   "source": [
    "test_query_1 = test_stage1[\"query\"].drop_duplicates()\n",
    "test_title_1 = test_stage1[\"title\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AE64AC21E706483D988191289BD09DBE"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del train,test_stage1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D55DA3AEF8A8432FA99716E91CC31338"
   },
   "outputs": [],
   "source": [
    "test_stage2 = pd.read_csv(\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\",header=None,names=[\"query_id\",\"query\",\"query_title_id\",\"title\"])\n",
    "test_stage2[\"title\"] = test_stage2[\"title\"].apply(lambda row : row.strip())\n",
    "print(test_stage2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DA400C92E25451FA5FAB4CE09ADA4B1"
   },
   "outputs": [],
   "source": [
    "test_query_2 = test_stage2[\"query\"].drop_duplicates()\n",
    "test_title_2 = test_stage2[\"title\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52F39669DF0D4968B17E3C87FE10AC97"
   },
   "outputs": [],
   "source": [
    "del test_stage2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "699F76F0BD9E48D687F193FD8BF320C9"
   },
   "outputs": [],
   "source": [
    "query = pd.concat([train_query,test_query_1,test_query_2],axis=0)\n",
    "title = pd.concat([train_title,test_title_1,test_title_2],axis=0)\n",
    "print(query.shape)\n",
    "print(title.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09E76950D3EB4B138E435F72C52A8D8C"
   },
   "outputs": [],
   "source": [
    "query = query.drop_duplicates()\n",
    "title = title.drop_duplicates()\n",
    "print(query.shape)\n",
    "print(title.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5EE9C623F8E44FF89635308A7CE82B3"
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E754A83C1DB840E5843FB70E84EEA193"
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(query) + list(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E10CC5B286E34BE188860D59A6B40D02"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10EFD80C16314B38A39C9FC34EB94D54"
   },
   "outputs": [],
   "source": [
    "joblib.dump(tokenizer,mypath+\"All_Tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1A0181253594724B1861C0D464A9E86"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30A9A11E6BFF485F884DF90893B9AC6C",
    "mdEditEnable": false
   },
   "source": [
    "# 三、提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E13C39BA524C4082AD266100B1EB0D99"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import Levenshtein\n",
    "import difflib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import chain\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7C6ABEA062A4135A86B2BB8D88BFE60",
    "mdEditEnable": false
   },
   "source": [
    "### 训练集和测试集1特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "295B20423C4948E785C441834A9C763D"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/kesci/work/train_last_100million.csv\")#,nrows=20000000)\n",
    "print(train.shape)\n",
    "test = pd.read_csv(\"/home/kesci/input/bytedance/test_final_part1.csv\",names=[\"query_id\",\"query\",\"query_title_id\",\"title\"])\n",
    "test[\"title\"] = test[\"title\"].strip()\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1DA063164B940B58D0EF2A67B9ED09F"
   },
   "outputs": [],
   "source": [
    "train_feature = pd.DataFrame()\n",
    "test_feature = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF828C509BA54A6686D38DF930F82874",
    "mdEditEnable": false
   },
   "source": [
    "### 1 length、count等统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "848062235CA04D3A82F1563534A49D06"
   },
   "outputs": [],
   "source": [
    "# ----- length 特征 -----\n",
    "for feature in ['query', 'title']:\n",
    "    print('计算' + feature  + '长度')\n",
    "    train_feature[feature + \"_length\"] = train[feature].apply(lambda row : len(row.split()))\n",
    "train_feature[\"query_title_abs\"] = abs(train_feature[\"query_length\"] - train_feature[\"title_length\"])\n",
    "train_feature[\"query_title_ratio\"] = train_feature[\"query_title_abs\"]/ (train_feature[\"query_length\"] + train_feature[\"title_length\"])\n",
    "\n",
    "for feature in ['query', 'title']:\n",
    "    print('计算' + feature  + '长度')\n",
    "    test_feature[feature + \"_length\"] = test[feature].apply(lambda row : len(row.split()))\n",
    "test_feature[\"query_title_abs\"] = abs(test_feature[\"query_length\"] - test_feature[\"title_length\"])\n",
    "test_feature[\"query_title_ratio\"] = test_feature[\"query_title_abs\"]/ (test_feature[\"query_length\"] + test_feature[\"title_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDAAB7C97BC9461CB1A2EE32A3D57ED6"
   },
   "outputs": [],
   "source": [
    "#---- click 特征 ----\n",
    "list_click_feature = ['query', 'title']\n",
    "# 计算某特征单次点击\n",
    "for feature in list_click_feature:\n",
    "    print('计算' + feature + '点击次数')\n",
    "    train_feature[feature + '_click'] = train.groupby(feature)[feature].transform('count')\n",
    "# 部分二元交叉点击\n",
    "train_feature['query_title_click'] = train.groupby(['query', 'title']).query.transform('count')\n",
    "train_feature['title_query_click'] = train.groupby(['query', 'title']).title.transform('count')\n",
    "\n",
    "##############################################3\n",
    "for feature in list_click_feature:\n",
    "    print('计算' + feature + '点击次数')\n",
    "    test_feature[feature + '_click'] = test.groupby(feature)[feature].transform('count')\n",
    "# 部分二元交叉点击\n",
    "test_feature['query_title_click'] = test.groupby(['query', 'title']).query.transform('count')\n",
    "test_feature['title_query_click'] = test.groupby(['query', 'title']).title.transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6797627DC1CB4E6584251BF3D9772141"
   },
   "outputs": [],
   "source": [
    "# ----- nunique 特征 -----\n",
    "train_feature[\"query_nunique_title\"] = train.groupby(\"query\")[\"title\"].transform(\"nunique\")\n",
    "train_feature[\"title_nunique_title\"] = train.groupby(\"title\")[\"query\"].transform(\"nunique\")\n",
    "\n",
    "test_feature[\"query_nunique_title\"] = test.groupby(\"query\")[\"title\"].transform(\"nunique\")\n",
    "test_feature[\"title_nunique_title\"] = test.groupby(\"title\")[\"query\"].transform(\"nunique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "755707C654254FBD841993C7EDB876F8"
   },
   "outputs": [],
   "source": [
    "all_feature = pd.concat([train_feature,test_feature],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4210CDB7176448283ACD7147A9BDD95"
   },
   "outputs": [],
   "source": [
    "np.save(\"/home/kesci/work/drift_data/count_feature_last_100m_split.npy\",all_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3894BCD89BA4D3F876FEA890D945920",
    "mdEditEnable": false
   },
   "source": [
    "### 2 编辑距离、fuzzy特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3B6B401C1309413EB2ABA6C6E29069FB"
   },
   "outputs": [],
   "source": [
    "test[\"label\"] = -1\n",
    "all_data = pd.concat([train,test],axis=0)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "549E23979ACD4B59A76890C95D6A221E"
   },
   "outputs": [],
   "source": [
    "del train,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "290E22249187426C9F7DB60802C36DAF"
   },
   "outputs": [],
   "source": [
    "all_feature = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83BA5A58EFF149F6B89B17886C16433E"
   },
   "outputs": [],
   "source": [
    "def levenshtein_feat(func, query=None, title=None):\n",
    "    a = query\n",
    "    b = title\n",
    "    n = 16\n",
    "    l = len(a) if isinstance(query, pd.Series) else len(b)\n",
    "    def tmp(query,title):\n",
    "        return [func(q, t) for q, t in zip(query, title)]\n",
    "    res = Parallel(n_jobs=n)(delayed(tmp)(a[int(i*l/n):int((i+1)*l/n)], b[int(i*l/n):int((i+1)*l/n)]) for i in range(n))\n",
    "    \n",
    "    return list(chain(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BA23D4C23B94E479973DD01680EB424"
   },
   "outputs": [],
   "source": [
    "query = all_data[\"query\"]\n",
    "title = all_data[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1F2CDB49EEF74F8F976BC1823A9FF683"
   },
   "outputs": [],
   "source": [
    "similarity_func = [Levenshtein.ratio, Levenshtein.distance]\n",
    "\n",
    "for func in similarity_func:\n",
    "    print('计算query与title_' + func.__name__  + '相似度')\n",
    "    all_feature['query_title_levenshtein_' + func.__name__] = levenshtein_feat(func,query,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4D7580A11A7464C86B1726FE4C6D643"
   },
   "outputs": [],
   "source": [
    "def fuzz_feat(func, query=None, title=None):\n",
    "    a = query\n",
    "    b = title\n",
    "    n = 16\n",
    "    l = len(a) if isinstance(query, pd.Series) else len(b)\n",
    "    def tmp(query,title):\n",
    "        return [func(q, t) for q, t in zip(query, title)]\n",
    "    res = Parallel(n_jobs=n)(delayed(tmp)(a[int(i*l/n):int((i+1)*l/n)], b[int(i*l/n):int((i+1)*l/n)]) for i in range(n))\n",
    "    \n",
    "    return list(chain(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EFD156C2A3C475589D4F5B5D017F171"
   },
   "outputs": [],
   "source": [
    "fuzz_func = [fuzz.QRatio, fuzz.WRatio, fuzz.partial_ratio, fuzz.partial_token_set_ratio,\n",
    "                fuzz.partial_token_sort_ratio, fuzz.token_set_ratio, fuzz.token_sort_ratio]\n",
    "for func in fuzz_func:\n",
    "    print('计算query与title_' + func.__name__ )\n",
    "    all_feature['query_title_fuzz_' + func.__name__] = fuzz_feat(func,query,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99CA59BFF85C4FC8992191D2E651AF1F"
   },
   "outputs": [],
   "source": [
    "np.save(\"/home/kesci/work/drift_data/fuzzy_feature_last_100m.npy\",all_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EF9308216FD487781CB0E5A72317300",
    "mdEditEnable": false
   },
   "source": [
    "### 3 embedding相似度特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBD7C4222DE7470CA1043CF69C283EC1"
   },
   "outputs": [],
   "source": [
    "all_feature = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9E815EA84F8427487C39AF60AD5960D"
   },
   "outputs": [],
   "source": [
    "query = train[\"query\"]\n",
    "title = train[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "506721A27D674CA3A4368D78E9E83B86"
   },
   "outputs": [],
   "source": [
    "def diff_feat(query=None, title=None):\n",
    "    a = query\n",
    "    b = title\n",
    "    n = 14\n",
    "    l = len(a) if isinstance(query, pd.Series) else len(b)\n",
    "    def tmp(query,title):\n",
    "        aa = []\n",
    "        seq = difflib.SequenceMatcher()\n",
    "        for q, t in zip(query, title):\n",
    "            seq.set_seqs(q, t)\n",
    "            aa.append(seq.ratio())\n",
    "            \n",
    "        return aa\n",
    "    res = Parallel(n_jobs=n)(delayed(tmp)(a[int(i*l/n):int((i+1)*l/n)], b[int(i*l/n):int((i+1)*l/n)]) for i in range(n))\n",
    "    \n",
    "    return list(chain(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "766E5450164F478BB32028476728D582"
   },
   "outputs": [],
   "source": [
    "all_feature[\"diff_ratio\"] = diff_feat(query, title)\n",
    "all_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6028B3C63D804CCB80A5ABD1E3F387BF"
   },
   "outputs": [],
   "source": [
    "def embedding_feat(func, query=None, title=None,model=None):\n",
    "    a = query\n",
    "    b = title\n",
    "    n = 14\n",
    "    l = len(a) if isinstance(query, pd.Series) else len(b)\n",
    "    def tmp(query,title):\n",
    "        aa = []\n",
    "        for q, t in zip(query, title):\n",
    "            row1 = q.split()\n",
    "            row2 = t.split()\n",
    "            tokens1 = [token for token in row1 if token in model]\n",
    "            tokens2 = [token for token in row2 if token in model]\n",
    "            if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "                aa.append(0)\n",
    "                continue\n",
    "            embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=None).reshape(1, -1)\n",
    "            embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=None).reshape(1, -1)\n",
    "            aa.append(func(embedding1,embedding2))\n",
    "        return aa\n",
    "    res = Parallel(n_jobs=n)(delayed(tmp)(a[int(i*l/n):int((i+1)*l/n)], b[int(i*l/n):int((i+1)*l/n)]) for i in range(n))\n",
    "    return list(chain(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "549AE49A148E489B943E4FC318132E9E"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"/home/kesci/work/drift_data/drift_word2vec_v100.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4794198362FC494D8D871D4AFC73FEDE"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "embedding_func = [braycurtis, cosine, canberra,euclidean, cityblock]\n",
    "for func in embedding_func:\n",
    "    print('计算query与title_' + func.__name__ )\n",
    "    all_feature['query_title_embedding_' + func.__name__] = embedding_feat(func,query,title,model)\n",
    "    print(\"Time : \",(time.time() - t0)/60 )\n",
    "    t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DA9F4D16A5C349C781F4AE353521B6D3"
   },
   "outputs": [],
   "source": [
    "np.save(\"/home/kesci/work/drift_data/stage2_embedding_feature_last_100m.npy\",all_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13D26FC2A67D455C96C40922755573B5",
    "mdEditEnable": false
   },
   "source": [
    "### 测试集1和测试集2提取特征（这里再提取一次测试集1是为了验证和上面顺序保持一致的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5942F3D59391427ABDF8A40EDE928AE6"
   },
   "outputs": [],
   "source": [
    "test_stage1 = pd.read_csv(\"/home/kesci/input/bytedance/test_final_part1.csv\",names=[\"query_id\",\"query\",\"query_title_id\",\"title\"])\n",
    "test_stage1[\"title\"] = test_stage1[\"title\"].apply(lambda row : row.strip())\n",
    "print(test_stage1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86BCDB240CF4467D81EDBD72BA5E5F9C"
   },
   "outputs": [],
   "source": [
    "test_stage2 = pd.read_csv(\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\",names=[\"query_id\",\"query\",\"query_title_id\",\"title\"])\n",
    "test_stage2[\"title\"] = test_stage2[\"title\"].apply(lambda row : row.strip())\n",
    "print(test_stage2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D7F85147B02445C8ED42D021C5A8B95",
    "mdEditEnable": false
   },
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8453D2706FA047508DDA035A20DBB33C"
   },
   "outputs": [],
   "source": [
    "test1_feature = pd.DataFrame()\n",
    "test2_feature = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1A909931E8B04225949D788B8DCF0EBF"
   },
   "outputs": [],
   "source": [
    "# ----- length 特征 -----\n",
    "for feature in ['query', 'title']:\n",
    "    print('计算' + feature  + '长度')\n",
    "    test1_feature[feature + \"_length\"] = test_stage1[feature].apply(lambda row : len(row.split()))\n",
    "test1_feature[\"query_title_abs\"] = abs(test1_feature[\"query_length\"] - test1_feature[\"title_length\"])\n",
    "test1_feature[\"query_title_ratio\"] = test1_feature[\"query_title_abs\"]/ (test1_feature[\"query_length\"] + test1_feature[\"title_length\"])\n",
    "\n",
    "for feature in ['query', 'title']:\n",
    "    print('计算' + feature  + '长度')\n",
    "    test2_feature[feature + \"_length\"] = test_stage2[feature].apply(lambda row : len(row.split()))\n",
    "test2_feature[\"query_title_abs\"] = abs(test2_feature[\"query_length\"] - test2_feature[\"title_length\"])\n",
    "test2_feature[\"query_title_ratio\"] = test2_feature[\"query_title_abs\"]/ (test2_feature[\"query_length\"] + test2_feature[\"title_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D9E225539794C6583CF0F631AA6FA97"
   },
   "outputs": [],
   "source": [
    "#---- click 特征 ----\n",
    "list_click_feature = ['query', 'title']\n",
    "# 计算某特征单次点击\n",
    "for feature in list_click_feature:\n",
    "    print('计算' + feature + '点击次数')\n",
    "    test1_feature[feature + '_click'] = test_stage1.groupby(feature)[feature].transform('count')\n",
    "# 部分二元交叉点击\n",
    "test1_feature['query_title_click'] = test_stage1.groupby(['query', 'title']).query.transform('count')\n",
    "test1_feature['title_query_click'] = test_stage1.groupby(['query', 'title']).title.transform('count')\n",
    "\n",
    "##############################################3\n",
    "for feature in list_click_feature:\n",
    "    print('计算' + feature + '点击次数')\n",
    "    test2_feature[feature + '_click'] = test_stage2.groupby(feature)[feature].transform('count')\n",
    "# 部分二元交叉点击\n",
    "test2_feature['query_title_click'] = test_stage2.groupby(['query', 'title']).query.transform('count')\n",
    "test2_feature['title_query_click'] = test_stage2.groupby(['query', 'title']).title.transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50EE520231934C938301B2323E1EE018"
   },
   "outputs": [],
   "source": [
    "# ----- nunique 特征 -----\n",
    "test1_feature[\"query_nunique_title\"] = test_stage1.groupby(\"query\")[\"title\"].transform(\"nunique\")\n",
    "test1_feature[\"title_nunique_title\"] = test_stage1.groupby(\"title\")[\"query\"].transform(\"nunique\")\n",
    "\n",
    "test2_feature[\"query_nunique_title\"] = test_stage2.groupby(\"query\")[\"title\"].transform(\"nunique\")\n",
    "test2_feature[\"title_nunique_title\"] = test_stage2.groupby(\"title\")[\"query\"].transform(\"nunique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6F9B4E0B41AE4533AA27D5CBBC280CEA"
   },
   "outputs": [],
   "source": [
    "all_test_feature = pd.concat([test1_feature,test2_feature],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DF3AD3AC856C4B3D9395D24F5B0C7541"
   },
   "outputs": [],
   "source": [
    "np.save(\"/home/kesci/work/drift_data/Test_count_feature.npy\",all_test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8892DEFD50B45058AF0B6C6520EEBDB",
    "mdEditEnable": false
   },
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D46B04A6C6724AC599F3308B9A1FA885"
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([test_stage1,test_stage2],axis=0)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD7E79818EEF417483259B96098C034C"
   },
   "outputs": [],
   "source": [
    "all_test_feature = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "353E1280D66744AF9181A4616984C441"
   },
   "outputs": [],
   "source": [
    "del test_stage1,test_stage2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56097C1B8A43456A8CE0306BC495BE1B"
   },
   "outputs": [],
   "source": [
    "def levenshtein_feat(func, query=None, title=None):\n",
    "    a = query\n",
    "    b = title\n",
    "    n = 16\n",
    "    l = len(a) if isinstance(query, pd.Series) else len(b)\n",
    "    def tmp(query,title):\n",
    "        return [func(q, t) for q, t in zip(query, title)]\n",
    "    res = Parallel(n_jobs=n)(delayed(tmp)(a[int(i*l/n):int((i+1)*l/n)], b[int(i*l/n):int((i+1)*l/n)]) for i in range(n))\n",
    "    \n",
    "    return list(chain(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DC68EB6F5084C7783ADA6A3D6A25F2C"
   },
   "outputs": [],
   "source": [
    "query = all_data[\"query\"]\n",
    "title = all_data[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95958C12CE144E2E80A3D6934494C348"
   },
   "outputs": [],
   "source": [
    "similarity_func = [Levenshtein.ratio, Levenshtein.distance]\n",
    "\n",
    "for func in similarity_func:\n",
    "    print('计算query与title_' + func.__name__  + '相似度')\n",
    "    all_test_feature['query_title_levenshtein_' + func.__name__] = levenshtein_feat(func,query,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08D56720D88540FEAF25F1FC3EA8132E"
   },
   "outputs": [],
   "source": [
    "def fuzz_feat(func, query=None, title=None):\n",
    "    a = query\n",
    "    b = title\n",
    "    n = 16\n",
    "    l = len(a) if isinstance(query, pd.Series) else len(b)\n",
    "    def tmp(query,title):\n",
    "        return [func(q, t) for q, t in zip(query, title)]\n",
    "    res = Parallel(n_jobs=n)(delayed(tmp)(a[int(i*l/n):int((i+1)*l/n)], b[int(i*l/n):int((i+1)*l/n)]) for i in range(n))\n",
    "    \n",
    "    return list(chain(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FF7F428FED3465B9487F29A45D607D7"
   },
   "outputs": [],
   "source": [
    "fuzz_func = [fuzz.QRatio, fuzz.WRatio, fuzz.partial_ratio, fuzz.partial_token_set_ratio,\n",
    "                fuzz.partial_token_sort_ratio, fuzz.token_set_ratio, fuzz.token_sort_ratio]\n",
    "for func in fuzz_func:\n",
    "    print('计算query与title_' + func.__name__ )\n",
    "    all_test_feature['query_title_fuzz_' + func.__name__] = fuzz_feat(func,query,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AF39F5BEA891449B993D7134FA010BB6"
   },
   "outputs": [],
   "source": [
    "np.save(\"/home/kesci/work/drift_data/Test_fuzzy_feature.npy\",all_test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8A362AFF7504B7580DFE9E56FC0327A",
    "mdEditEnable": false
   },
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4699CA4B3BBC44DF8662DE02D864057D"
   },
   "outputs": [],
   "source": [
    "all_test_feature = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8C5C0DAF97BB4BE18DC94F64925B562F"
   },
   "outputs": [],
   "source": [
    "def diff_feat(query=None, title=None):\n",
    "    a = query\n",
    "    b = title\n",
    "    n = 14\n",
    "    l = len(a) if isinstance(query, pd.Series) else len(b)\n",
    "    def tmp(query,title):\n",
    "        aa = []\n",
    "        seq = difflib.SequenceMatcher()\n",
    "        for q, t in zip(query, title):\n",
    "            seq.set_seqs(q, t)\n",
    "            aa.append(seq.ratio())\n",
    "            \n",
    "        return aa\n",
    "    res = Parallel(n_jobs=n)(delayed(tmp)(a[int(i*l/n):int((i+1)*l/n)], b[int(i*l/n):int((i+1)*l/n)]) for i in range(n))\n",
    "    \n",
    "    return list(chain(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61F3FD7E3CB24144A4FC8B1954C2E672"
   },
   "outputs": [],
   "source": [
    "all_test_feature[\"diff_ratio\"] = diff_feat(query, title)\n",
    "all_test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0547E3C2DE8B4EB39B3170E3B948455D"
   },
   "outputs": [],
   "source": [
    "def embedding_feat(func, query=None, title=None,model=None):\n",
    "    a = query\n",
    "    b = title\n",
    "    n = 14\n",
    "    l = len(a) if isinstance(query, pd.Series) else len(b)\n",
    "    def tmp(query,title):\n",
    "        aa = []\n",
    "        for q, t in zip(query, title):\n",
    "            row1 = q.split()\n",
    "            row2 = t.split()\n",
    "            tokens1 = [token for token in row1 if token in model]\n",
    "            tokens2 = [token for token in row2 if token in model]\n",
    "            if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "                aa.append(0)\n",
    "                continue\n",
    "            embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=None).reshape(1, -1)\n",
    "            embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=None).reshape(1, -1)\n",
    "            aa.append(func(embedding1,embedding2))\n",
    "        return aa\n",
    "    res = Parallel(n_jobs=n)(delayed(tmp)(a[int(i*l/n):int((i+1)*l/n)], b[int(i*l/n):int((i+1)*l/n)]) for i in range(n))\n",
    "    return list(chain(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FE59C9DBE274406CB4D965869BBEF97F"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"/home/kesci/work/drift_data/drift_word2vec_v100.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E14E79EF0B3A4A659EDF687F121E23BC"
   },
   "outputs": [],
   "source": [
    "# embedding_func = [braycurtis, cosine, canberra,euclidean, cityblock]\n",
    "# for func in embedding_func:\n",
    "#     print('计算query与title_' + func.__name__ )\n",
    "#     all_test_feature['query_title_embedding_' + func.__name__] = embedding_feat(func,query,title,model)\n",
    "    \n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "embedding_func = [braycurtis, cosine, canberra,euclidean, cityblock]\n",
    "for func in embedding_func:\n",
    "    print('计算query与title_' + func.__name__ )\n",
    "    all_test_feature['query_title_embedding_' + func.__name__] = embedding_feat(func,query,title,model)\n",
    "    print(\"Time : \",(time.time() - t0)/60 )\n",
    "    t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5740493F87C47B4AA38212CEA4694BF"
   },
   "outputs": [],
   "source": [
    "np.save(\"/home/kesci/work/drift_data/Test_embedding_feature.npy\",all_test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1769D2812094B578B9C0AB3969ADDA1",
    "mdEditEnable": false
   },
   "source": [
    "# 四、训练lightgbm1，a榜：0.591+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "928B16E001EC4BE286254715F1668B92"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, Levenshtein, time\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import difflib\n",
    "#import swifter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4A4511F71CD46488D152F20A29EB386"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/kesci/work/train_last_100million.csv\")#,nrows=20000000)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5B12137903AC4AA7A55BF34DC5AF5B06"
   },
   "outputs": [],
   "source": [
    "labels = train[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2A75B49883D4815A6AC28AB59D90E4D"
   },
   "outputs": [],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F50E58A32B39405187249E878F7C224F"
   },
   "outputs": [],
   "source": [
    "#训练集\n",
    "count = np.load(\"/home/kesci/work/drift_data/count_feature_last_100m_split.npy\")\n",
    "print(count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FE6E4550DEC43E198040A7A5F7B3EC6"
   },
   "outputs": [],
   "source": [
    "#测试集\n",
    "test_count = np.load(\"/home/kesci/work/drift_data/Test_count_feature.npy\")\n",
    "print(test_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "455CE5A8F0164CA59ECA12A9B41B8AF8"
   },
   "outputs": [],
   "source": [
    "#测试集\n",
    "test_count = np.load(\"/home/kesci/work/drift_data/Test_count_feature.npy\")\n",
    "print(test_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "141A2CF8AAD4459A8281B330ABE9DF14"
   },
   "outputs": [],
   "source": [
    "#测试集\n",
    "test_fuzzy = np.load(\"/home/kesci/work/drift_data/Test_fuzzy_feature.npy\")\n",
    "print(test_fuzzy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4CD66E114C2459C8526B8F04E53A2DE"
   },
   "outputs": [],
   "source": [
    "#新训练集 (没有diff_ratio)\n",
    "embed = np.load(\"/home/kesci/work/drift_data/stage2_embedding_feature_last_100m.npy\")\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5D6DD101B0A4A498A683B39263894E3"
   },
   "outputs": [],
   "source": [
    "#测试集\n",
    "test_embed = np.load(\"/home/kesci/work/drift_data/Test_embedding_feature.npy\")\n",
    "print(test_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C96177C2D2D04C8282512B658925FAFE"
   },
   "outputs": [],
   "source": [
    "#合并特征\n",
    "\n",
    "# train\n",
    "train = np.hstack((count[:100000000],fuzzy[:100000000]))\n",
    "print(train.shape)\n",
    "train = np.hstack((train,embed))\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE676F76043F4DE4B8BB73599B6AF976"
   },
   "outputs": [],
   "source": [
    "# valid\n",
    "test1 = np.hstack((test_count[:20000000],test_fuzzy[:20000000]))\n",
    "test1 = np.hstack((test1,test_embed[:20000000]))\n",
    "print(test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5E7E85BA7AB4654969B3D5E39F2B88C"
   },
   "outputs": [],
   "source": [
    "test2 = np.hstack((test_count[20000000:],test_fuzzy[20000000:]))\n",
    "test2 = np.hstack((test2,test_embed[20000000:]))\n",
    "print(test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A34DB1B6B7824E2380F0A6036B5DDA6A"
   },
   "outputs": [],
   "source": [
    "# 划分训练集与验证集\n",
    "valid_feat = train[90000000:]\n",
    "train_feat = train[:90000000]\n",
    "valid_label = labels[90000000:]\n",
    "train_label = labels[:90000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "803105765FFD45F78BF5F29518988666"
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "        boosting_type = 'gbdt', num_leaves = 64, reg_alpha = 5, reg_lambda = 5,\n",
    "        n_estimators = 1500, objective = 'binary',subsample = 0.7, colsample_bytree = 0.7, \n",
    "        subsample_freq = 1,learning_rate = 0.05, random_state = 2019,\n",
    "        n_jobs=14)\n",
    "\n",
    "clf.fit(train_feat, train_label, eval_set = [(train_feat, train_label),(valid_feat,valid_label)],\n",
    "        early_stopping_rounds = 100,verbose = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B241E7F8D939496382027B05C298A50F"
   },
   "outputs": [],
   "source": [
    "cols=[\n",
    "        \"query_length\",\"title_length\",\"query_title_abs\",\"query_title_ratio\",\"query_click\",\"title_click\",\n",
    "        \"query_title_click\",\"title_query_click\",\"query_nunique_title\",\"title_nunique_title\",\n",
    "        \n",
    "        \"levenshtein_ratio\",\"levenshtein_distence\",\"fuzz_QRatio\",\"fuzz_WRatio\",\n",
    "        \"fuzz_partial_ratio\",\"fuzz_partial_token_set_ratio\",\"fuzz_partial_token_sort_ratio\",\n",
    "        \"fuzz_token_set_ratio\",\"fuzz_token_sort_ratio\",\n",
    "        \n",
    "        \"diff_ratio\",\"embedding_braycurtis\",\"embedding_cosine\",\"embedding_canberra\",\"embedding_euclidean\",\n",
    "        \"embedding_cityblock\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "959443C787CE47AA8C72949BFC669390"
   },
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame()\n",
    "feature_importance[\"feature\"] = cols\n",
    "feature_importance[\"importances\"] = clf.feature_importances_\n",
    "feature_importance.sort_values(\"importances\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CB26E0BDFCF440448ACC5695F33A0EEE"
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(clf,'/home/kesci/work/drift_data/dump_lgb_model.pkl')\n",
    "clf = joblib.load('/home/kesci/work/drift_data/dump_lgb_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0F1594C432EF4D5B8AE0E40F4C5111AF"
   },
   "outputs": [],
   "source": [
    "#valid 0.691\n",
    "preds = clf.predict_proba(valid)[:,1]\n",
    "roc_auc_score(valid_label,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91E7256C2C6B402AAA17D6413F362EE8",
    "mdEditEnable": false
   },
   "source": [
    "### 测试集1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61216CCFF92E4E44B181542D39820A28"
   },
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(test1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "620CA7F0C8024AD5974981D504F8191C"
   },
   "outputs": [],
   "source": [
    "path = \"/home/kesci/input/bytedance/\"\n",
    "test_path = path+\"test_final_part1.csv\"\n",
    "test = pd.read_csv(test_path,header=None,names=[\"query_id\",\"query\",\"query_title_id\",\"title\"])\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A08CD08C22C04C898872227CFAE86155"
   },
   "outputs": [],
   "source": [
    "submit = test[[\"query_id\",\"query_title_id\"]]\n",
    "submit[\"prediction\"] = probs\n",
    "submit.to_csv(\"stage1_lgb_submit_v0.csv\",index=False,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CF6D1DEF5A744A3867F6569AB63BC97",
    "mdEditEnable": false
   },
   "source": [
    "### 测试集2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0C9EFB7CB3B44358A4A4381A30B47AE"
   },
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(test2)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5694A093C97A4E3AB076F39754F11130"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\",header=None,names=[\"query_id\",\"query\",\"query_title_id\",\"title\"])\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "221D99D1834C4F0C96A70EDF786FDB20"
   },
   "outputs": [],
   "source": [
    "submit = test[[\"query_id\",\"query_title_id\"]]\n",
    "submit[\"prediction\"] = probs\n",
    "submit.to_csv(\"stage2_lgb_submit_v0.csv\",index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "183C7629B13749EE8AB257AD087BE1D4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8E22EA913694F3DB911DDD54B8497D2",
    "mdEditEnable": false
   },
   "source": [
    "# 五、nn1 Textcnn模型，a榜：0.603+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D848EEDE06D40688ABC3445CF93636B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras \n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "869690AD890648758B409C7E6DFD4C13"
   },
   "outputs": [],
   "source": [
    "path = \"/home/kesci/input/bytedance/\"\n",
    "mypath = \"/home/kesci/work/drift_data/\"\n",
    "\n",
    "batch_size = 256\n",
    "num_words = 700000\n",
    "embed_size = 300\n",
    "max_sequence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09A4209D810648378D3A5373AD4290A7"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/kesci/work/train_last_100million.csv\")#,nrows=20000000)\n",
    "print(train.shape)\n",
    "test1 = pd.read_csv(\"/home/kesci/input/bytedance/test_final_part1.csv\",header=None,names=[\"query_id\",\"query\",\"query_title_id\",\"title\"])\n",
    "test1[\"title\"] = test1[\"title\"].apply(lambda row : row.strip())\n",
    "print(test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F48A09ADFFD84D9A8580EF66644364B5"
   },
   "outputs": [],
   "source": [
    "tokenizer = joblib.load(\"/home/kesci/work/drift_data/All_Tokenizer.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3E28DB783FBC467CBC0EBC512C000650"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "'''\n",
    "Time :  27.89003413915634\n",
    "Time :  38.88456140359243\n",
    "'''\n",
    "\n",
    "train_query = sequence.pad_sequences(tokenizer.texts_to_sequences(list(train[\"query\"])), maxlen=max_sequence_length)\n",
    "print(\"Time : \",(time.time() -t0)/60)\n",
    "t0 = time.time()\n",
    "train_title = sequence.pad_sequences(tokenizer.texts_to_sequences(list(train[\"title\"])), maxlen=max_sequence_length)\n",
    "print(\"Time : \",(time.time() -t0)/60)\n",
    "t0 = time.time()\n",
    "test_query = sequence.pad_sequences(tokenizer.texts_to_sequences(list(test1[\"query\"])), maxlen=max_sequence_length)\n",
    "print(\"Time : \",(time.time() -t0)/60)\n",
    "t0 = time.time()\n",
    "test_title = sequence.pad_sequences(tokenizer.texts_to_sequences(list(test1[\"title\"])), maxlen=max_sequence_length)\n",
    "print(\"Time : \",(time.time() -t0)/60)\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "223103F6DD184D94864A24058A8E8DA8"
   },
   "outputs": [],
   "source": [
    "labels = train[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02D398EB48F24C7E87FDB70016C7F439"
   },
   "outputs": [],
   "source": [
    "valid_query = train_query[90000000:]\n",
    "valid_title = train_title[90000000:]\n",
    "\n",
    "train_query = train_query[:90000000]\n",
    "train_title = train_title[:90000000]\n",
    "\n",
    "train_labels = labels[:90000000]\n",
    "valid_labels = labels[90000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AD197567E90D408996359711D9CD2AA2"
   },
   "outputs": [],
   "source": [
    "train_y = to_categorical(train_labels)\n",
    "valid_y = to_categorical(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90E1DC35FC7348308ACBEA542E03C4FB"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "num_words = len(word_index)#min(num_words, len(word_index))\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2D2DE1B8BC514BB28D21E013C5D131AF"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"/home/kesci/work/drift_data/drift_word2vec_v100.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5A855AB129A341F2B573910169BD9CF4"
   },
   "outputs": [],
   "source": [
    "ss = 0\n",
    "embedding_matrix = np.zeros((num_words+1, embed_size))\n",
    "print(len(word_index.items()))\n",
    "for word, i in word_index.items():\n",
    "    if word in model.wv.vocab:\n",
    "        ss += 1\n",
    "        if i >= num_words:\n",
    "            break\n",
    "        embedding_matrix[i] = model.wv[word]\n",
    "    else:\n",
    "        unk_vec = np.random.random(embed_size) * 0.5\n",
    "        unk_vec = unk_vec - unk_vec.mean()\n",
    "        embedding_matrix[i] = unk_vec\n",
    "print(ss)\n",
    "print(ss/num_words)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6304948E8AF47198478735495A8A65B"
   },
   "outputs": [],
   "source": [
    "#训练集特征读取\n",
    "#训练集\n",
    "count = np.load(\"/home/kesci/work/drift_data/count_feature_last_100m_split.npy\")\n",
    "print(count.shape)\n",
    "train_count = count[:100000000]\n",
    "del count\n",
    "gc.collect()\n",
    "\n",
    "#训练集\n",
    "fuzzy = np.load(\"/home/kesci/work/drift_data/fuzzy_feature_last_100m.npy\")\n",
    "fuzzy.shape\n",
    "train_fuzzy = fuzzy[:100000000]\n",
    "print(train_fuzzy.shape)\n",
    "del fuzzy\n",
    "gc.collect()\n",
    "\n",
    "embed = np.load(\"/home/kesci/work/drift_data/stage2_embedding_feature_last_100m.npy\")\n",
    "print(embed.shape)\n",
    "train_embed = embed[:100000000]\n",
    "print(train_embed.shape)\n",
    "del embed\n",
    "gc.collect()\n",
    "\n",
    "diff_ratio = np.load(\"/home/kesci/work/drift_data/diff_ratio_feature_100m.npy\")\n",
    "diff_ratio.shape\n",
    "train_diff = diff_ratio[:100000000]\n",
    "del diff_ratio\n",
    "gc.collect()\n",
    "\n",
    "train_embed = np.hstack((train_diff,train_embed))\n",
    "print(train_embed.shape)\n",
    "\n",
    "\n",
    "train = np.hstack((train_count,train_fuzzy))\n",
    "print(train.shape)\n",
    "train = np.hstack((train,train_embed))\n",
    "print(train.shape)\n",
    "\n",
    "del train_count,train_fuzzy,train_embed\n",
    "gc.collect()\n",
    "#np.save(\"/home/kesci/work/drift_data/demo_train_feat.npy\",train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3C2BA361DD3A46B6A405974241CB5FAC"
   },
   "outputs": [],
   "source": [
    "# 测试集test1\n",
    "test_count = np.load(\"/home/kesci/work/drift_data/Test_count_feature.npy\")\n",
    "print(test_count.shape)\n",
    "test_fuzzy = np.load(\"/home/kesci/work/drift_data/Test_fuzzy_feature.npy\")\n",
    "print(test_fuzzy.shape)\n",
    "valid = np.hstack((test_count[:20000000],test_fuzzy[:20000000]))\n",
    "del test_count,test_fuzzy\n",
    "gc.collect()\n",
    "\n",
    "test_embed = np.load(\"/home/kesci/work/drift_data/Test_embedding_feature.npy\")\n",
    "print(test_embed.shape)\n",
    "\n",
    "# valid\n",
    "#valid = np.hstack((test_count[:20000000],test_fuzzy[:20000000]))\n",
    "valid = np.hstack((valid,test_embed[:20000000]))\n",
    "print(valid.shape)\n",
    "del test_embed\n",
    "gc.collect()\n",
    "#np.save(\"/home/kesci/work/drift_data/demo_valid_feat.npy\",valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1F4F79011A224E0E9437ED3034C819A2"
   },
   "outputs": [],
   "source": [
    "train_feat = train[:90000000]\n",
    "valid_feat = train[90000000:]\n",
    "print(train_feat.shape)\n",
    "print(valid_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4B15C87D482A444F8FBD00667611FB95"
   },
   "outputs": [],
   "source": [
    "test_feat = valid\n",
    "print(test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87DB0D40AFA24B0788E0E35804231965"
   },
   "outputs": [],
   "source": [
    "feat_shape = train_feat.shape[1]\n",
    "feat_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DB230F7EB4444D78293FEE4BCB0412C"
   },
   "outputs": [],
   "source": [
    "def build_model(emb_matrix):\n",
    "    \n",
    "    embed = Embedding(\n",
    "        input_dim=emb_matrix.shape[0],\n",
    "        output_dim=emb_matrix.shape[1],\n",
    "        weights=[emb_matrix],\n",
    "        input_length=max_sequence_length,\n",
    "        trainable=False)\n",
    "\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    seq1 = Input(shape=(max_sequence_length,))\n",
    "    seq2 = Input(shape=(max_sequence_length,))\n",
    "    \n",
    "    emb1 = embed(seq1)\n",
    "    emb2 = embed(seq2)\n",
    "\n",
    "    conv1a = conv1(emb1)\n",
    "    glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "    max1a = GlobalMaxPooling1D()(conv1a)\n",
    "\n",
    "    conv1b = conv1(emb2)\n",
    "    glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "    max1b = GlobalMaxPooling1D()(conv1b)\n",
    "\n",
    "    conv2a = conv2(emb1)\n",
    "    glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "    max2a = GlobalMaxPooling1D()(conv2a)\n",
    "\n",
    "    conv2b = conv2(emb2)\n",
    "    glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "    max2b = GlobalMaxPooling1D()(conv2b)\n",
    "\n",
    "    conv3a = conv3(emb1)\n",
    "    glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "    max3a = GlobalMaxPooling1D()(conv3a)\n",
    "\n",
    "    conv3b = conv3(emb2)\n",
    "    glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "    max3b = GlobalMaxPooling1D()(conv3b)\n",
    "\n",
    "    conv4a = conv4(emb1)\n",
    "    glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "    max4a = GlobalMaxPooling1D()(conv4a)\n",
    "\n",
    "    conv4b = conv4(emb2)\n",
    "    glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "    max4b = GlobalMaxPooling1D()(conv4b)\n",
    "\n",
    "    conv5a = conv5(emb1)\n",
    "    glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "    max5a = GlobalMaxPooling1D()(conv5a)\n",
    "  \n",
    "    conv5b = conv5(emb2)\n",
    "    glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "    max5b = GlobalMaxPooling1D()(conv5b)\n",
    "\n",
    "    conv6a = conv6(emb1)\n",
    "    glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "    max6a = GlobalMaxPooling1D()(conv6a)\n",
    "\n",
    "    conv6b = conv6(emb2)\n",
    "    glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "    max6b = GlobalMaxPooling1D()(conv6b)\n",
    "\n",
    "    mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "    mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "    mergec = concatenate([max1a, max2a, max3a, max4a, max5a, max6a])\n",
    "    merged = concatenate([max1b, max2b, max3b, max4b, max5b, max6b])\n",
    "\n",
    "    diff_ave = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    mul_ave = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    diff_max = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergec, merged])\n",
    "    mul_max = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergec, merged])\n",
    "    \n",
    "    #feature 输入\n",
    "    feature_input = Input(shape=(feat_shape,))\n",
    "    feature_dense = Dense(128, activation='relu')(feature_input)\n",
    "    feature_dense = Dropout(0.2)(feature_dense)\n",
    "    feature_dense = Dense(128, activation='relu')(feature_dense)\n",
    "    feature_dense = BatchNormalization()(feature_dense)\n",
    "\n",
    "    merge = concatenate([diff_ave, mul_ave,diff_max,mul_max,feature_dense])\n",
    "    \n",
    "    print(merge.shape)\n",
    "    x = BatchNormalization()(merge)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    pred = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[seq1, seq2,feature_input], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23E21E3A680C424A8D944991070384AA"
   },
   "outputs": [],
   "source": [
    "ft = build_model(embedding_matrix)\n",
    "\n",
    "early_stop = EarlyStopping(patience=3)\n",
    "\n",
    "check_point = ModelCheckpoint('/home/kesci/work/drift_data/models/Stage2_Improved_TextCNN_Epoch_{epoch:02d}.hdf5', monitor=\"val_loss\", \n",
    "                    mode=\"min\", save_best_only=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8E2CFD770E948BCAED66F847F0DB307"
   },
   "outputs": [],
   "source": [
    " hist = ft.fit([train_query, train_title,train_feat],\n",
    "                     train_y, \n",
    "                     validation_data=([valid_query, valid_title,valid_feat], valid_y),\n",
    "                     epochs=1,\n",
    "                     batch_size=512,\n",
    "                     shuffle=True,\n",
    "                     callbacks=[early_stop, check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14FD1789E285465598D85AF582A0A7D7"
   },
   "outputs": [],
   "source": [
    "ft.load_weights(\"/home/kesci/work/drift_data/models/Stage2_Improved_TextCNN_Epoch_01.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EEAED53B422437C9174D89DD6432A23"
   },
   "outputs": [],
   "source": [
    "preds_1 = ft.predict([valid_query,valid_title,valid_feat], batch_size=512, verbose=1)[:,1]\n",
    "roc_auc_score(valid_labels,preds_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A141481CF326451AA482EB3C08D54C0C"
   },
   "outputs": [],
   "source": [
    "test_probs = ft.predict([test_query,test_title,test_feat],batch_size=512*4,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA8C49AC27EB4E5BA5EEB14950D4B4D5"
   },
   "source": [
    "### 测试集2预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3FE62AA8B61446C8390337A14E3FDAE"
   },
   "outputs": [],
   "source": [
    "# 测试集test2\n",
    "test_count = np.load(\"/home/kesci/work/drift_data/Test_count_feature.npy\")\n",
    "print(test_count.shape)\n",
    "test_fuzzy = np.load(\"/home/kesci/work/drift_data/Test_fuzzy_feature.npy\")\n",
    "print(test_fuzzy.shape)\n",
    "test2_feat = np.hstack((test_count[20000000:],test_fuzzy[20000000:]))\n",
    "del test_count,test_fuzzy\n",
    "gc.collect()\n",
    "\n",
    "test_embed = np.load(\"/home/kesci/work/drift_data/Test_embedding_feature.npy\")\n",
    "print(test_embed.shape)\n",
    "\n",
    "# valid\n",
    "#valid = np.hstack((test_count[:20000000],test_fuzzy[:20000000]))\n",
    "test2_feat = np.hstack((test2_feat,test_embed[20000000:]))\n",
    "print(test2_feat.shape)\n",
    "del test_embed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7E40B5FEF9C4892BC623195A7013D4F"
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "test2_query = sequence.pad_sequences(tokenizer.texts_to_sequences(list(test2[\"query\"])), maxlen=max_sequence_length)\n",
    "print(\"Time : \",(time.time() -t0)/60)\n",
    "t0 = time.time()\n",
    "test2_title = sequence.pad_sequences(tokenizer.texts_to_sequences(list(test2[\"title\"])), maxlen=max_sequence_length)\n",
    "print(\"Time : \",(time.time() -t0)/60)\n",
    "\n",
    "np.save(mypath+\"test2_query.npy\",test2_query)\n",
    "np.save(mypath+\"test2_title.npy\",test2_title)\n",
    "\n",
    "test2_query = np.load(mypath+\"test2_query.npy\")\n",
    "test2_title = np.load(mypath+\"test2_title.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BA1C9D01618D451281D21E1AA45A6A5C"
   },
   "outputs": [],
   "source": [
    "test2_probs = ft.predict([test2_query,test2_title,test2_feat],batch_size=512*4,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B567253757D24B7282754356859643E4"
   },
   "outputs": [],
   "source": [
    "test2 = pd.read_csv(\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\",header=None,names=[\"query_id\",\"query\",\"query_title_id\",\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9E5E4AEADAC446FA81E5ECADBD0F3B96"
   },
   "outputs": [],
   "source": [
    "submit = test2[[\"query_id\",\"query_title_id\"]]\n",
    "submit[\"probs\"] = test2_probs[:,1]\n",
    "submit.to_csv(\"stage2_improved_textcnn_feat_submit_v0.csv\",index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3C1CC03767E14423863DD3A20D878A6E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C44C7B96805439E87C31F6F031F7248",
    "mdEditEnable": false,
    "scrolled": false
   },
   "source": [
    "#  六、nn2 BiLstm_Fasttext 模型，a榜：0.603-0.605（没有线上提交）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36AA1C7AEACB43218F2B331E548D7CDA"
   },
   "outputs": [],
   "source": [
    "#bilstm_fasttext\n",
    "def model3(embedding_matrix):\n",
    "     \n",
    "    query = Input(shape=(max_sequence_length,),name=\"query\")\n",
    "    title = Input(shape=(max_sequence_length,),name=\"title\")\n",
    "    \n",
    "    embedding = Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                            output_dim=embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False,name=\"embedding\")\n",
    "    \n",
    "    q_embed = embedding(query)\n",
    "    q_embed = SpatialDropout1D(0.2)(q_embed)\n",
    "    \n",
    "    print(q_embed.shape)\n",
    "    \n",
    "    t_embed = embedding(title)\n",
    "    t_embed = SpatialDropout1D(0.2)(t_embed)\n",
    "    \n",
    "    print(t_embed.shape)\n",
    "    \n",
    "    share_BiLSTM1 = Bidirectional(CuDNNLSTM(100, return_sequences=True))\n",
    "    share_BiLSTM2= Bidirectional(CuDNNLSTM(100, return_sequences=True))\n",
    "    \n",
    "    # 两层 Bilstm\n",
    "    q_lstm1 = share_BiLSTM1(q_embed)\n",
    "    q_lstm = share_BiLSTM2(q_lstm1)\n",
    "    \n",
    "    q_lstm = concatenate([q_embed, q_lstm1, q_lstm])\n",
    "    print(\"q_lstm : \",q_lstm.shape)\n",
    "    \n",
    "    t_lstm1 = share_BiLSTM1(t_embed)\n",
    "    t_lstm = share_BiLSTM2(t_lstm1)\n",
    "    \n",
    "    t_lstm =  concatenate([t_embed, t_lstm1, t_lstm])\n",
    "    \n",
    "    q_max = GlobalMaxPool1D()(q_lstm)\n",
    "    q_ave = GlobalAveragePooling1D()(q_lstm)\n",
    "    t_max = GlobalMaxPool1D()(t_lstm)\n",
    "    t_ave = GlobalAveragePooling1D()(t_lstm)\n",
    "        \n",
    "    print(q_max.shape)\n",
    "    print(q_ave.shape)\n",
    "    print(t_max.shape)\n",
    "    print(t_ave.shape)\n",
    "    \n",
    "    # 取差和积\n",
    "    maxpool_diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(700,))([q_max, t_max])\n",
    "    maxpool_mul = Lambda(lambda x: x[0] * x[1], output_shape=(700 ,))([q_max, t_max])\n",
    "    \n",
    "    avepool_diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(700,))([q_ave, t_ave])\n",
    "    avepool_mul = Lambda(lambda x: x[0] * x[1], output_shape=(700 ,))([q_ave, t_ave])\n",
    "    \n",
    "    #feature 输入\n",
    "    feature_input = Input(shape=(feat_shape,))\n",
    "    feature_dense = Dense(128, activation='relu')(feature_input)\n",
    "    feature_dense = Dropout(0.2)(feature_dense)\n",
    "    feature_dense = Dense(128, activation='relu')(feature_dense)\n",
    "    feature_dense = BatchNormalization()(feature_dense)\n",
    "    \n",
    "    merged = concatenate([ maxpool_diff,maxpool_mul ,avepool_diff , avepool_mul,feature_dense])\n",
    "    print(merged.shape)\n",
    "    \n",
    "    x = BatchNormalization()(merged)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    pred = Dense(2, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[query, title,feature_input], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15921213A6C34D12AE5D679D886C2ED4"
   },
   "outputs": [],
   "source": [
    "model = model3(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5C258F4393A4D638FA3CC8DACBCABC8"
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience=3)\n",
    "check_point = ModelCheckpoint('/home/kesci/work/drift_data/models/Stage2_BiLSTM_FastText_Epoch1.hdf5', monitor=\"val_loss\", \n",
    "                    mode=\"min\", save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "321DECFF1D494F748DBE79894EDD77E4"
   },
   "outputs": [],
   "source": [
    " history = model.fit([train_query, train_title,train_feat],train_y, \n",
    "                      validation_data=([valid_query, valid_title,valid_feat], valid_y),\n",
    "                      epochs=1,batch_size=512,shuffle=True,\n",
    "                      callbacks=[early_stop, check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5B70C64032C74C63803BECF0383247C5",
    "mdEditEnable": false
   },
   "source": [
    "### 测试集2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "421C3A1A71BD4F6B930C2F1CF73B3A08"
   },
   "outputs": [],
   "source": [
    "test2_probs = model.predict([test2_query,test2_title,test2_feat],batch_size=512*4,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7C09D2BB8D85446C8294463F145EBF64"
   },
   "outputs": [],
   "source": [
    "submit = test2[[\"query_id\",\"query_title_id\"]]\n",
    "submit[\"probs\"] = test2_probs[:,1]\n",
    "submit.to_csv(\"stage2_LSTM_FastText_feat_submit_Epoch1.csv\",index=False,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C55F3E8E77CE49B98A2275D3369DD3BF",
    "mdEditEnable": false
   },
   "source": [
    "# 七、nn3 BiLstm_TextCNN 模型，a榜：0.610+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E26F1D1F9C164BA1877C0B7E0125E880"
   },
   "outputs": [],
   "source": [
    "# 模型2 BiLSTM—CNN \n",
    "def model2(embedding_matrix):\n",
    "     \n",
    "    query = Input(shape=(max_sequence_length,),name=\"query\")\n",
    "    title = Input(shape=(max_sequence_length,),name=\"title\")\n",
    "    \n",
    "    embedding = Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                            output_dim=embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False,name=\"embedding\")\n",
    "    \n",
    "    q_embed = embedding(query)\n",
    "    q_embed = SpatialDropout1D(0.2)(q_embed)\n",
    "    \n",
    "    print(q_embed.shape)\n",
    "    \n",
    "    t_embed = embedding(title)\n",
    "    t_embed = SpatialDropout1D(0.2)(t_embed)\n",
    "    \n",
    "    print(t_embed.shape)\n",
    "    \n",
    "    share_BiLSTM1 = Bidirectional(CuDNNLSTM(100, return_sequences=True))\n",
    "    share_BiLSTM2= Bidirectional(CuDNNLSTM(100, return_sequences=True))\n",
    "    \n",
    "    # 两层 Bilstm\n",
    "    q_lstm1 = share_BiLSTM1(q_embed)\n",
    "    q_lstm = share_BiLSTM2(q_lstm1)\n",
    "    \n",
    "    q_lstm = concatenate([q_embed, q_lstm1, q_lstm])\n",
    "    print(\"q_lstm : \",q_lstm.shape)\n",
    "    \n",
    "    t_lstm1 = share_BiLSTM1(t_embed)\n",
    "    t_lstm = share_BiLSTM2(t_lstm1)\n",
    "    \n",
    "    t_lstm =  concatenate([t_embed, t_lstm1, t_lstm])\n",
    "    \n",
    "    # 卷积层\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=128, kernel_size=5, padding='same', activation='relu')\n",
    "\n",
    "    # 1\n",
    "    q_conv1 = conv1(q_lstm)\n",
    "    q_maxpool1 = GlobalMaxPool1D()(q_conv1)\n",
    "    q_avepool1 = GlobalAveragePooling1D()(q_conv1)\n",
    "    \n",
    "    q_conv2 = conv2(q_lstm)\n",
    "    q_maxpool2 = GlobalMaxPool1D()(q_conv2)\n",
    "    q_avepool2 = GlobalAveragePooling1D()(q_conv2)\n",
    "    \n",
    "    q_conv3 = conv3(q_lstm)\n",
    "    q_maxpool3 = GlobalMaxPool1D()(q_conv3)\n",
    "    q_avepool3 = GlobalAveragePooling1D()(q_conv3)\n",
    "    \n",
    "    q_conv4 = conv4(q_lstm)\n",
    "    q_maxpool4 = GlobalMaxPool1D()(q_conv4)\n",
    "    q_avepool4 = GlobalAveragePooling1D()(q_conv4)\n",
    "    \n",
    "    q_conv5 = conv5(q_lstm)\n",
    "    q_maxpool5 = GlobalMaxPool1D()(q_conv5)\n",
    "    q_avepool5 = GlobalAveragePooling1D()(q_conv5)\n",
    "    \n",
    "    print(\"conv1 : \", q_conv1.shape)\n",
    "    print(\"maxpool : \",q_maxpool1.shape)\n",
    "    print(\"avepool : \",q_avepool1.shape)\n",
    "    \n",
    "    q_max = concatenate([q_maxpool1,q_maxpool2,q_maxpool3,q_maxpool4,q_maxpool5])\n",
    "    q_ave = concatenate([q_avepool1,q_avepool2,q_avepool3,q_avepool4,q_avepool5])\n",
    "    \n",
    "    print(\"query maxpooling : \",q_max.shape)\n",
    "    print(\"query averagepooling : \",q_ave.shape)\n",
    "    \n",
    "    t_conv1 = conv1(t_lstm)\n",
    "    t_maxpool1 = GlobalMaxPool1D()(t_conv1)\n",
    "    t_avepool1 = GlobalAveragePooling1D()(t_conv1)\n",
    "        \n",
    "    t_conv2 = conv2(t_lstm)\n",
    "    t_maxpool2 = GlobalMaxPool1D()(t_conv2)\n",
    "    t_avepool2 = GlobalAveragePooling1D()(t_conv2)\n",
    "        \n",
    "    t_conv3 = conv3(t_lstm)\n",
    "    t_maxpool3 = GlobalMaxPool1D()(t_conv3)\n",
    "    t_avepool3 = GlobalAveragePooling1D()(t_conv3)\n",
    "        \n",
    "    t_conv4 = conv4(t_lstm)\n",
    "    t_maxpool4 = GlobalMaxPool1D()(t_conv4)\n",
    "    t_avepool4 = GlobalAveragePooling1D()(t_conv4)\n",
    "        \n",
    "    t_conv5 = conv5(t_lstm)\n",
    "    t_maxpool5 = GlobalMaxPool1D()(t_conv5)\n",
    "    t_avepool5 = GlobalAveragePooling1D()(t_conv5)\n",
    "        \n",
    "    #maxpool 和 average 合并\n",
    "    q_max = concatenate([q_maxpool1,q_maxpool2,q_maxpool3,q_maxpool4,q_maxpool5])\n",
    "    q_ave = concatenate([q_avepool1,q_avepool2,q_avepool3,q_avepool4,q_avepool5])\n",
    "    \n",
    "    t_max = concatenate([t_maxpool1,t_maxpool2,t_maxpool3,t_maxpool4,t_maxpool5])\n",
    "    t_ave = concatenate([t_avepool1,t_avepool2,t_avepool3,t_avepool4,t_avepool5])\n",
    "        \n",
    "    # 取差和积\n",
    "    maxpool_diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(5 * 128,))([q_max, t_max])\n",
    "    maxpool_mul = Lambda(lambda x: x[0] * x[1], output_shape=(5 * 128 ,))([q_max, t_max])\n",
    "    \n",
    "    avepool_diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(5 * 128,))([q_ave, t_ave])\n",
    "    avepool_mul = Lambda(lambda x: x[0] * x[1], output_shape=(5 * 128 ,))([q_ave, t_ave])\n",
    "    \n",
    "    #feature 输入\n",
    "    feature_input = Input(shape=(feat_shape,))\n",
    "    feature_dense = Dense(128, activation='relu')(feature_input)\n",
    "    feature_dense = Dropout(0.2)(feature_dense)\n",
    "    feature_dense = Dense(128, activation='relu')(feature_dense)\n",
    "    feature_dense = BatchNormalization()(feature_dense)\n",
    "    \n",
    "    merged = concatenate([ maxpool_diff,maxpool_mul ,avepool_diff , avepool_mul,feature_dense])\n",
    "    print(merged.shape)\n",
    "    \n",
    "    \n",
    "    x = BatchNormalization()(merged)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    pred = Dense(2, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[query, title,feature_input], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F408193E8BBD4A8C8BBA60332AB19F35"
   },
   "outputs": [],
   "source": [
    "model = model2(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1397592B39AF44449EC8EBF1049BA27C"
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience=3)\n",
    "check_point = ModelCheckpoint('/home/kesci/work/drift_data/models/Stage2_LSTM_CNN_Epoch_{epoch:02d}.hdf5', monitor=\"val_loss\", \n",
    "                    mode=\"min\", save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6B7D0B16AA854BC7821E233684B37CFD"
   },
   "outputs": [],
   "source": [
    " history = model.fit([train_query, train_title,train_feat],train_y, \n",
    "                      validation_data=([valid_query, valid_title,valid_feat], valid_y),\n",
    "                      epochs=3,batch_size=512,shuffle=True,\n",
    "                      callbacks=[early_stop, check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71D9E021CF10443A817D2C53CCAC428B"
   },
   "outputs": [],
   "source": [
    "preds_1 = model.predict([valid_query,valid_title,valid_feat], batch_size=512, verbose=1)[:,1]\n",
    "roc_auc_score(valid_labels,preds_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B13FF94C8D50430F806AA83AEBEB7F20"
   },
   "outputs": [],
   "source": [
    "test2_probs = model.predict([test2_query,test2_title,test2_feat],batch_size=512*4,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4065DABD21C64BEEB5A1870B82C0BAAE"
   },
   "outputs": [],
   "source": [
    "submit = test2[[\"query_id\",\"query_title_id\"]]\n",
    "submit[\"probs\"] = test2_probs[:,1]\n",
    "submit.to_csv(\"stage2_LSTM_CNN_feat_submit_Epoch3.csv\",index=False,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8573508044F042378472AF8F1C3C7F6F",
    "mdEditEnable": false
   },
   "source": [
    "# 八、lightgbm2 a榜：0.590+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "951BF247E88C4BA385611E3750D0FEB2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 显示cell运行时长\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "080C766E2462425084601250521B1B30",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: pyemd in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from pyemd)\n",
      "Requirement already up-to-date: pandas in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already up-to-date: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already up-to-date: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already up-to-date: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already up-to-date: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas)\n",
      "time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple fuzzywuzzy\n",
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pyemd\n",
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "43128FF5DA2B47379283476F5ED28E45",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.06 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from array import array\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import tqdm\n",
    "from scipy.linalg import norm\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra\n",
    "from nltk import word_tokenize\n",
    "from joblib import Parallel, delayed\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "86BC274163FB4ACA80B6E839910518C6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.65 ms\n"
     ]
    }
   ],
   "source": [
    "# tf相似度\n",
    "def my_tf_similarity(s1, s2):\n",
    "    c1 = Counter(s1)\n",
    "    c2 = Counter(s2)\n",
    "    union = set(s1) | set(s2)\n",
    "    vectors = np.zeros((2, len(union)))\n",
    "    for i, w in enumerate(union):\n",
    "        vectors[0, i] = c1[w]\n",
    "        vectors[1, i] = c2[w]\n",
    "    return np.dot(vectors[0], vectors[1]) / (norm(vectors[0]) * norm(vectors[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "C516DA8FC2F142B78A5C1036C7BA599C",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.52 ms\n"
     ]
    }
   ],
   "source": [
    "# 杰卡德系数1\n",
    "def my_jaccard1(s1, s2):\n",
    "    \"\"\"\n",
    "    考虑出现次数\n",
    "    \"\"\"\n",
    "    a = set(s1)\n",
    "    b = set(s2)\n",
    "    numerator = denominator = 0\n",
    "    for i in a & b:\n",
    "        c1, c2 = s1.count(i), s2.count(i)\n",
    "        numerator += c1 if c1 < c2 else c2 \n",
    "    for i in a | b:\n",
    "        c1, c2 = s1.count(i), s2.count(i)\n",
    "        denominator += c1 if c1 > c2 else c2 \n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "4DDAA6A848AD46C588C43B945F5ECF90",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.03 ms\n"
     ]
    }
   ],
   "source": [
    "# 杰卡德系数2\n",
    "def my_jaccard2(s1, s2):\n",
    "    \"\"\"\n",
    "    不考虑出现次数\n",
    "    \"\"\"    \n",
    "    a = set(s1)\n",
    "    b = set(s2)\n",
    "    return len(a & b) / len(a | b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     2
    ],
    "id": "7E17475A28ED40169F92FAE020E5269B",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.99 ms\n"
     ]
    }
   ],
   "source": [
    "# 编辑距离\n",
    "def levenshtein(seq1, seq2, max_dist=-1):\n",
    "    \"\"\"\n",
    "    seq1, seq2:空格分隔的句子\n",
    "    \"\"\"\n",
    "    if seq1 == seq2:\n",
    "        return 0\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "    # if max_dist >= 0 and abs(len1 - len2) > max_dist:\n",
    "    #     return -1\n",
    "    # if len1 == 0:\n",
    "    #     return len2\n",
    "    # if len2 == 0:\n",
    "    #     return len1\n",
    "    if len1 < len2:\n",
    "        len1, len2 = len2, len1\n",
    "        seq1, seq2 = seq2, seq1\n",
    "    \n",
    "    column = array('L', range(len2 + 1))\n",
    "    \n",
    "    for x in range(1, len1 + 1):\n",
    "        column[0] = x\n",
    "        last = x - 1\n",
    "        for y in range(1, len2 + 1):\n",
    "            old = column[y]\n",
    "            cost = int(seq1[x - 1] != seq2[y - 1])\n",
    "            column[y] = min(column[y] + 1, column[y - 1] + 1, last + cost)\n",
    "            last = old\n",
    "        if max_dist >= 0 and min(column) > max_dist:\n",
    "            return -1\n",
    "    \n",
    "    if max_dist >= 0 and column[len2] > max_dist:\n",
    "        # stay consistent, even if we have the exact distance\n",
    "        return -1\n",
    "    return column[len2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "6863F3BEDA07455EA0D878AB1D3F757B",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.2 ms\n"
     ]
    }
   ],
   "source": [
    "#sorensen 相似性系数\n",
    "def sorensen(seq1, seq2):\n",
    "\tset1, set2 = set(seq1), set(seq2)\n",
    "\treturn 1 - (2 * len(set1 & set2) / float(len(set1) + len(set2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "3DE351E224484BF7B12CD71C49FD4394",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.09 ms\n"
     ]
    }
   ],
   "source": [
    "# 生成句向量\n",
    "def sent2vec(words):\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model.wv[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / len(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "DADD04C7D0A842D98FBDC3739E381D09",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "def qauc(x):\n",
    "    try:\n",
    "        score = roc_auc_score(x.label, x.pred)\n",
    "    except:\n",
    "        score = -1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "C7F6B107FFB743AD9E6D2104A863E33E",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.02 ms\n"
     ]
    }
   ],
   "source": [
    "# title中包含的query中的词数\n",
    "def n_include_word(s1, s2):\n",
    "    n = 0\n",
    "    for w in s1:\n",
    "        if w in s2:\n",
    "            n += 1\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "7B3FB12C4C1E4B3A9840B860C4697D3A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.15 ms\n"
     ]
    }
   ],
   "source": [
    "# 最长子串长度\n",
    "def find_lcsubstr(s1, s2): \n",
    "\tm=[[0 for i in range(len(s2)+1)]  for j in range(len(s1)+1)]  #生成0矩阵，为方便后续计算，比字符串长度多了一列\n",
    "\tmmax=0   #最长匹配的长度\n",
    "\tp=0  #最长匹配对应在s1中的最后一位\n",
    "\tfor i in range(len(s1)):\n",
    "\t\tfor j in range(len(s2)):\n",
    "\t\t\tif s1[i]==s2[j]:\n",
    "\t\t\t\tm[i+1][j+1]=m[i][j]+1\n",
    "\t\t\t\tif m[i+1][j+1]>mmax:\n",
    "\t\t\t\t\tmmax=m[i+1][j+1]\n",
    "\t\t\t\t\tp=i+1\n",
    "\treturn mmax   #返回最长子串及其长度 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "1F024B5F211144FB8EC2205D5DCE7A37",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.9 ms\n"
     ]
    }
   ],
   "source": [
    "# 最长子序列长度\n",
    "def find_lcseque(s1, s2): \n",
    "\t# 生成字符串长度加1的0矩阵，m用来保存对应位置匹配的结果\n",
    "\tm = [ [ 0 for x in range(len(s2)+1) ] for y in range(len(s1)+1) ] \n",
    "\t# d用来记录转移方向\n",
    "\td = [ [ None for x in range(len(s2)+1) ] for y in range(len(s1)+1) ] \n",
    "    \n",
    "\tfor p1 in range(len(s1)): \n",
    "\t\tfor p2 in range(len(s2)): \n",
    "\t\t\tif s1[p1] == s2[p2]:            #字符匹配成功，则该位置的值为左上方的值加1\n",
    "\t\t\t\tm[p1+1][p2+1] = m[p1][p2]+1\n",
    "\t\t\t\td[p1+1][p2+1] = 'ok'          \n",
    "\t\t\telif m[p1+1][p2] > m[p1][p2+1]:  #左值大于上值，则该位置的值为左值，并标记回溯时的方向\n",
    "\t\t\t\tm[p1+1][p2+1] = m[p1+1][p2] \n",
    "\t\t\t\td[p1+1][p2+1] = 'left'          \n",
    "\t\t\telse:                           #上值大于左值，则该位置的值为上值，并标记方向up\n",
    "\t\t\t\tm[p1+1][p2+1] = m[p1][p2+1]   \n",
    "\t\t\t\td[p1+1][p2+1] = 'up'         \n",
    "\t(p1, p2) = (len(s1), len(s2)) \n",
    "\ts = [] \n",
    "\twhile m[p1][p2]:    #不为None时\n",
    "\t\tc = d[p1][p2]\n",
    "\t\tif c == 'ok':   #匹配成功，插入该字符，并向左上角找下一个\n",
    "\t\t\ts.append(s1[p1-1])\n",
    "\t\t\tp1-=1\n",
    "\t\t\tp2-=1 \n",
    "\t\tif c =='left':  #根据标记，向左找下一个\n",
    "\t\t\tp2 -= 1\n",
    "\t\tif c == 'up':   #根据标记，向上找下一个\n",
    "\t\t\tp1 -= 1\n",
    "\ts.reverse() \n",
    "\treturn len(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "5AD2174D6E27409B8A92F5055049FB09",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.69 ms\n"
     ]
    }
   ],
   "source": [
    "# 多进程并行提取特征\n",
    "def extract_feat(func, query=None, title=None, sent2vec=None):\n",
    "    a = query\n",
    "    b = title\n",
    "    n = 16\n",
    "    l = len(a) if isinstance(query, pd.Series) else len(b)\n",
    "    def tmp1(query, title):\n",
    "        return [func(q.split(), t.split()) for q, t in zip(query, title)]\n",
    "    def tmp2(query):\n",
    "        return [func(q.split()) for q in query]\n",
    "    def tmp3(title):\n",
    "        return [func(t.split()) for t in title]\n",
    "    def tmp4(query, title):\n",
    "        return [func(sent2vec(q.split()), sent2vec(t.split())) for q, t in zip(query, title)]\n",
    "    def tmp5(query):\n",
    "        return [func(sent2vec(q.split())) for q in query]\n",
    "    def tmp6(title):\n",
    "        return [func(sent2vec(t.split())) for t in title]\n",
    "    if isinstance(query, pd.Series) and isinstance(title, pd.Series):\n",
    "        tmp = tmp4 if sent2vec else tmp1\n",
    "        res = Parallel(n_jobs=n)(delayed(tmp)(a[int(i*l/n):int((i+1)*l/n)], b[int(i*l/n):int((i+1)*l/n)]) for i in range(n))\n",
    "    elif isinstance(query, pd.Series):\n",
    "        tmp = tmp5 if sent2vec else tmp2\n",
    "        res = Parallel(n_jobs=n)(delayed(tmp)(query[int(i*l/n):int((i+1)*l/n)]) for i in range(n))\n",
    "    elif isinstance(title, pd.Series):\n",
    "        tmp = tmp6 if sent2vec else tmp3\n",
    "        res = Parallel(n_jobs=n)(delayed(tmp)(title[int(i*l/n):int((i+1)*l/n)]) for i in range(n))\n",
    "    return list(chain(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "81E1F3A33257429F84EC12CF5C698845",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "# 最终测试集\n",
    "test = pd.read_csv(\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\", \n",
    "                    names=['query_id','query','query_title_id','title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "E16BD2A0AD1A420CA49CB0ED771C97FA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 917 µs\n"
     ]
    }
   ],
   "source": [
    "# 前5kw训练集\n",
    "train = pd.read_csv(\"/home/kesci/input/bytedance/train_final.csv\", nrows=50000000,\n",
    "                    names=['query_id','query','query_title_id','title', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "40549D3D0DFA40199BB3930DB42D0DB5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.09 s\n"
     ]
    }
   ],
   "source": [
    "query = pd.concat([train['query'], test['query']], ignore_index=True)\n",
    "title = pd.concat([train.title, test.title], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "58F33C4DEC2E448F8451871B4EA6F85F",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.1 ms\n"
     ]
    }
   ],
   "source": [
    "feat = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "BEA2B90829E048508F9F2E847460FC2E",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 21 s\n"
     ]
    }
   ],
   "source": [
    "# query长度\n",
    "feat['len_query'] = extract_feat(len, query=query)\n",
    "# title长度\n",
    "feat['len_title'] = extract_feat(len, title=title)\n",
    "feat['diff_len'] = feat['len_title'] - feat['len_query']\n",
    "# 杰卡德系数\n",
    "feat['jaccard1'] = extract_feat(my_jaccard1, query, title)\n",
    "feat['jaccard2'] = extract_feat(my_jaccard2, query, title)\n",
    "# tf相似度\n",
    "feat['tf_similarity'] = extract_feat(my_tf_similarity, query, title)\n",
    "# 编辑距离\n",
    "feat['levenshtein'] = extract_feat(levenshtein, query, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "F12EF5A79F8F41999703329C2D6990EE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "titles_count = train.groupby('query_id').label.count()\n",
    "titles_count = pd.merge(train[['query_id']], titles_count.reset_index(), on='query_id', how='left')\n",
    "titles_count = titles_count.rename(columns={'label': 'titles_count'})\n",
    "titles_count2 = test.groupby('query_id').query_title_id.count()\n",
    "titles_count2 = pd.merge(test[['query_id']], titles_count2.reset_index(), on='query_id', how='left')\n",
    "titles_count2 = titles_count2.rename(columns={'query_title_id': 'titles_count'})\n",
    "titles_count = pd.concat([titles_count, titles_count2], ignore_index=True)\n",
    "# 当前query下的title数量\n",
    "feat['titles_count'] = titles_count.titles_count\n",
    "# title中包含的query中的词数\n",
    "feat['n_include_word'] = extract_feat(n_include_word, query, title)\n",
    "# 最长子串长度\n",
    "feat['len_lcsubstr'] = extract_feat(find_lcsubstr, query, title)\n",
    "# 最长子序列长度\n",
    "feat['len_lcseque'] = extract_feat(find_lcseque, query, title)\n",
    "feat['diff_n_include_word'] = feat.len_query - feat.n_include_word\n",
    "feat['diff_len_lcsubstr'] = feat.len_query - feat.len_lcsubstr\n",
    "feat['diff_len_lcseque'] = feat.len_query - feat.len_lcseque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "512BDFDE800C431482500402B7A9E3E3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 599 ms\n"
     ]
    }
   ],
   "source": [
    "# 当前title的序数\n",
    "feat['query_title_id'] = pd.concat([train['query_title_id'], test['query_title_id']], ignore_index=True)\n",
    "title_value_counts = title.value_counts().reset_index()\n",
    "title_value_counts.rename(columns={'index': 'title', 'title': 'title_value_counts'}, inplace=True)\n",
    "title_value_counts = pd.merge(pd.DataFrame(title), title_value_counts, on='title', how='left')\n",
    "# title出现次数\n",
    "feat['title_value_counts'] = title_value_counts['title_value_counts']\n",
    "#sorensen 相似性系数\n",
    "feat['sorensen_distance'] = extract_feat(sorensen, query, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "1784694918234155962A59FC08A626DD",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/opt/conda/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30min 7s\n"
     ]
    }
   ],
   "source": [
    "# 载入Word2Vec模型\n",
    "model = Word2Vec.load(\"/home/kesci/work/drift_data/drift_word2vec_v100.model\")\n",
    "# 余弦距离\n",
    "feat['cosine_distance'] = extract_feat(cosine, query, title, sent2vec)\n",
    "# 曼哈顿距离\n",
    "feat['cityblock_distance'] = extract_feat(cityblock, query, title, sent2vec)\n",
    "feat['canberra_distance'] = extract_feat(canberra, query, title, sent2vec)\n",
    "# 词移距离\n",
    "feat['wmd'] = extract_feat(model.wv.wmdistance, query, title)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5E853901938648648AD9D333DD7C0B74",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "# fuzz特征\n",
    "feat['fuzz_qratio'] = extract_feat(fuzz.QRatio, query, title)\n",
    "feat['fuzz_WRatio'] = extract_feat(fuzz.WRatio, query, title)\n",
    "feat['fuzz_partial_ratio'] = extract_feat(fuzz.partial_ratio, query, title)\n",
    "feat['fuzz_partial_token_set_ratio'] = extract_feat(fuzz.partial_token_set_ratio, query, title)\n",
    "feat['fuzz_partial_token_sort_ratio'] = extract_feat(fuzz.partial_token_sort_ratio, query, title)\n",
    "feat['fuzz_token_set_ratio'] = extract_feat(fuzz.token_set_ratio, query, title)\n",
    "feat['fuzz_token_sort_ratio'] = extract_feat(fuzz.token_sort_ratio, query, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "48249D8E509A418E9166F2ACEFC06C7A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14min 24s\n"
     ]
    }
   ],
   "source": [
    "feature_path = \"/home/kesci/work/feature_hao.h5\"\n",
    "feature.to_hdf(feature_path, 'feat', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A92BF6793E5F46D7B67207C07F57343A"
   },
   "outputs": [],
   "source": [
    "feature_path = \"/home/kesci/work/feature_hao.h5\"\n",
    "feat = pd.read_hdf(feature_path, \"feat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94CBADB372CF425987D03862FCC3F6B8"
   },
   "outputs": [],
   "source": [
    "train_sp = 50000000\n",
    "trainset = lgb.Dataset(feat[:train_sp].values, \n",
    "                       label=train.label.values,\n",
    "                       feature_name=feat.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82451A841ED64F7486F471ED5D79EB3D"
   },
   "outputs": [],
   "source": [
    "params={'boosting':'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'learning_rate': 0.06,\n",
    "        'max_depth': 6,\n",
    "        'num_leaves': 31,\n",
    "        'metric':'auc',\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'subsample_freq': 1,\n",
    "        'reg_alpha': 5,\n",
    "        'reg_lambda': 5,\n",
    "        'n_jobs':16\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EE081ADE7AD34354A08A357291B56F3B"
   },
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "num_rounds = 1000\n",
    "bst2 = lgb.train(params, trainset, num_rounds)\n",
    "# 保存模型\n",
    "bst2.save_model(\"/home/kesci/work/lgb_final.model\")\n",
    "# 载入模型\n",
    "bst2 = lgb.Booster(model_file=\"/home/kesci/work/lgb_final.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BE08A4ED07584DD38D2F7AE9CBEC2195"
   },
   "outputs": [],
   "source": [
    "del trainset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "336CCFE2D86446EDA9B641E30D4BD8F7"
   },
   "outputs": [],
   "source": [
    "y_pred = bst2.predict(feat[train_sp:])\n",
    "sub = test[['query_id','query_title_id']]\n",
    "sub['pred'] = y_pred\n",
    "sub.to_csv(\"/home/kesci/work/lgb_hao_final.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCE69A39693F4F73BD8697F900E0D656"
   },
   "source": [
    "# 九、lightgbm3 a榜:0.582+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C621DD518884F4E8F45A8E6FDD5D914"
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录\n",
    "!ls /home/kesci/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AE1CE47BCE714D3A82D49EAECB7C7A50"
   },
   "outputs": [],
   "source": [
    "# 查看个人持久化工作区文件\n",
    "!ls /home/kesci/work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F70F8D29A2EA4C409E06EE2DE8D16942"
   },
   "outputs": [],
   "source": [
    "# 查看当前kernerl下的package\n",
    "!pip list --format=columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84C0C745FB9E46C589648C39214BBEEA"
   },
   "outputs": [],
   "source": [
    "# 显示cell运行时长\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5A16B53A3EF54F2089BF0AB77F7EC6DD"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from array import array\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from collections import Counter\n",
    "from gensim.models import word2vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import gensim\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.similarities import WmdSimilarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_log(appname, lv=logging.INFO):\n",
    "    logger = logging.getLogger(appname)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s[line:%(lineno)d] - %(levelname)-4s: %(message)s')\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.formatter = formatter\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.setLevel(lv)\n",
    "\n",
    "    return logger\n",
    "\n",
    "train_path = '/home/kesci/input/bytedance/train_final.csv'\n",
    "test_path = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n",
    "train_text_feat_path = '/home/kesci/work/lmm/feature/train_text_feat.csv'\n",
    "test_text_feat_path = '/home/kesci/work/lmm/feature/test_text_feat.csv'\n",
    "train_cnt_feat_path = '/home/kesci/work/lmm/feature/train_cnt_feat.csv'\n",
    "test_cnt_feat_path = '/home/kesci/work/lmm/feature/test_cnt_feat.csv'\n",
    "train_semantic_feat_path = '/home/kesci/work/lmm/feature/train_semantic_feat.csv'\n",
    "test_semantic_feat_path = '/home/kesci/work/lmm/feature/test_semantic_feat.csv'\n",
    "train_w2v_semantic_feat_path = '/home/kesci/work/lmm/feature/train_w2v_semantic_feat.csv'\n",
    "test_w2v_semantic_feat_path = '/home/kesci/work/lmm/feature/test_w2v_semantic_feat.csv'\n",
    "prob_path = '/home/kesci/work/lmm/output/add_click.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5C7E0C8A29242EDAA7226DAC69045C1",
    "mdEditEnable": false
   },
   "source": [
    "# 多线程/多进程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3DCFF9A99F94F308CCA6DF40A395E21"
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "from functools import wraps\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "class ParallelMachine(object):\n",
    "    \"\"\"\n",
    "        并行机器, 并发执行若干任务\n",
    "\n",
    "        注: 多进程和多线程会导致内存占用大量增加, 所以避免负载中出现大对象的复制\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__func_mapping = []\n",
    "        self.__result_mapping = {}\n",
    "\n",
    "    @classmethod\n",
    "    def do(cls, o):\n",
    "        \"\"\"\n",
    "            多进程要求对象可以序列化, 闭包内的对象无法序列化, 因此存在此方法\n",
    "\n",
    "            用dill替换pickle\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info(f\"using {o[2]}\")\n",
    "        \n",
    "        start = time.time()\n",
    "        result = o[0](o[1])\n",
    "        end = time.time()\n",
    "        \n",
    "        return o[2], result  # label, func(data)\n",
    "\n",
    "    def job(self, mapping=None, handlers=None):\n",
    "        \"\"\"\n",
    "            标记一个函数位需要并发执行,\n",
    "\n",
    "            :param mapping: 入参和结果集的标签, 示例: {\"train\": train}\n",
    "            :param handlers: 同标签数据对应的预处理函数\n",
    "            :param before: 执行被标记函数前执行的函数\n",
    "            :param before_args: 前置函数入参\n",
    "            :param after: 执行被标记函数后执行的函数\n",
    "            :param after_args: 后置函数入参\n",
    "\n",
    "            示例\n",
    "            pm = ParallelMachine()\n",
    "\n",
    "            @pm.job(mapping={\"first_label\": first_data, \"second_label\", second_data})\n",
    "            def func(data):\n",
    "                # do something with data\n",
    "                pass\n",
    "                # return result\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        if mapping is None:\n",
    "            mapping = {}\n",
    "\n",
    "        def inner(func):\n",
    "            @wraps(func)\n",
    "            def wrapper(*args, **kwargs):\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "            for k, v in mapping.items():\n",
    "                if callable(func):\n",
    "                    label = func.__name__ + \"_\" + k\n",
    "                    self.__func_mapping.append((wrapper, v, label))\n",
    "                else:\n",
    "                    raise ValueError(f\"[{func}] is not callable\")\n",
    "            return wrapper\n",
    "\n",
    "        return inner\n",
    "\n",
    "    def mutilprocess(self, worker=(cpu_count() - 1), timeout=20):\n",
    "        \"\"\"\n",
    "            多进程执行被标记方法们\n",
    "            :param worker 进程数, 默认是CPU逻辑核心少一个\n",
    "            :param timeout 每个进程的最大执行时间, 默认20s\n",
    "    \n",
    "            注: 多进程必须在main进程下调用\n",
    "        \"\"\"\n",
    "\n",
    "        if __name__ == \"__main__\":\n",
    "            with ProcessPoolExecutor(max_workers=worker) as executor:\n",
    "                self.__result_mapping = {k: v for k, v in executor.map(self.do, self.__func_mapping, timeout=timeout)}\n",
    "        else:\n",
    "            raise EnvironmentError(\"Multiprocess must be under main process\")\n",
    "\n",
    "    @property\n",
    "    def result(self):\n",
    "        return self.__result_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AE1B6B6654C40F89E835D3E18784911"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logger = get_log('kesci_Final_')\n",
    "logger.info('读取训练集数据...')\n",
    "train = pd.read_csv(train_path, index_col=None, nrows=50000000,names=['query_id','query','query_title_id','title','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9A315B2F2B354E2D818FAA4B71018BC4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logger.info('读取测试集数据...')\n",
    "test = pd.read_csv(\"/home/kesci/input/bytedance/bytedance_contest.final_2.csv\", index_col=None, names=['query_id','query','query_title_id','title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8FF2065B14E414BB72D69A5BECEF181",
    "mdEditEnable": false
   },
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48B5FDFDE7194B4D8C94BBA03AA53A39",
    "mdEditEnable": false
   },
   "source": [
    "### 文本特征（跑完后须要重启kernel运行特征工程前所有cell再提取下一个特征）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FEB2EBE96B14BA6B30EC2ACF6124678"
   },
   "outputs": [],
   "source": [
    "def q_t_loc(data):\n",
    "    count = []\n",
    "    for i, row in data.iterrows():\n",
    "        lens = len(row['query'])\n",
    "        if row['query'] not in row['title']:\n",
    "            count.append(-1)\n",
    "        elif row['query'] in row['title'][:lens]:\n",
    "            # 判断是否在前\n",
    "            count.append(0)\n",
    "        elif row['query'] in row['title'][-lens:]:\n",
    "            # 判断是否在后\n",
    "            count.append(2)\n",
    "        else:\n",
    "            # 判断是否在中\n",
    "            count.append(1)\n",
    "    return count\n",
    "\n",
    "def q_t_index(data):\n",
    "    '''\n",
    "        查看query在title中出现的位置\n",
    "    '''\n",
    "    count = []\n",
    "    for i, row in data.iterrows():\n",
    "        if row['query'] not in row['title']:\n",
    "            count.append(-1)\n",
    "        elif row['query'] in row['title']:\n",
    "            t = ''.join(row['title'].split(' '))\n",
    "            q = ''.join(row['query'].split(' '))\n",
    "            q_index_t = t.index(q)\n",
    "            count.append(q_index_t)\n",
    "    return count\n",
    "\n",
    "\n",
    "def q_t_appear_feat(data):\n",
    "    '''\n",
    "        计算query是否完整出现在title中\n",
    "        计算query出现在title中的位置\n",
    "    '''\n",
    "    count = []\n",
    "    logger.info('计算query是否完整出现在title中...')\n",
    "    for i, row in data.iterrows():\n",
    "        if row['query'] in row['title']:\n",
    "            count.append(1)\n",
    "        else:\n",
    "            count.append(0)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00D66ED3847F43739E1D8C9AD88801DE"
   },
   "outputs": [],
   "source": [
    "# 初始化ParallelMachine\n",
    "# start\n",
    "pm = ParallelMachine() # 实例化pm对象 \n",
    "\n",
    "def areas(total, pieces):\n",
    "    return (((total * i)// pieces, (total * (i + 1)) // pieces) for i in range(pieces))\n",
    "    \n",
    "train_mapping = {f\"train_{i}\": train.iloc[area[0]:area[1], :] for i, area in enumerate(areas(train.shape[0], 14))}\n",
    "test_mapping = {f\"test_{i}\": test.iloc[area[0]:area[1], :] for i, area in enumerate(areas(test.shape[0], 14))}\n",
    "\n",
    "job_mapping = {**train_mapping, **test_mapping}\n",
    "# end\n",
    "\n",
    "@pm.job(job_mapping)\n",
    "def get_text_feat(data):\n",
    "    '''\n",
    "        提取文本特征:长度---长度差---index\n",
    "    '''\n",
    "    t_f = data[['query_id', 'query','query_title_id', 'title']]\n",
    "    \n",
    "    logger.info('提取query长度特征...')\n",
    "    t_f['query_len'] = [len(i) for i in t_f['query']]\n",
    "    logger.info('提取title长度特征...')\n",
    "    t_f['title_len'] = [len(i) for i in t_f['title']]\n",
    "    logger.info('提取query title长度差特征..')\n",
    "    t_f['query_title_len_differ'] = t_f['title_len'] - t_f['query_len']\n",
    "    logger.info('提取query是否在title中特征...')\n",
    "    t_f['q_t_is_appear'] = q_t_appear_feat(t_f[['query', 'title']])\n",
    "    \n",
    "    logger.info('提取query出现在title中的loc...')\n",
    "    t_f['q_t_loc_appear'] = q_t_loc(t_f[['query', 'title']])\n",
    "    logger.info('提取query出现在title中的index...')\n",
    "    t_f['q_t_loc_index'] = q_t_index(t_f[['query', 'title']])\n",
    "    \n",
    "    return t_f.drop(['query', 'title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B136C76E8C92415C8646E2985C18BAE2"
   },
   "outputs": [],
   "source": [
    "# 获取并行执行结果 \n",
    "# 获取并行执行结果 \n",
    "from functools import reduce\n",
    "\n",
    "pm.mutilprocess(worker=12, timeout=None) # 多进程\n",
    "\n",
    "def merge_parts(resultset, prefix, r):\n",
    "    return reduce(lambda d1, d2: d1.append(d2), (resultset[f\"{prefix}_{i}\"] for i in range(r)))\n",
    "\n",
    "# 文本特征 56min28s\n",
    "logger.info('提取训练集文本特征...')\n",
    "\n",
    "train_text_feat = merge_parts(pm.result, \"get_text_feat_train\", 14) # 执行结果为 函数名_给定标签\n",
    "# 28min34s\n",
    "\n",
    "logger.info('提取测试集文本特征...')\n",
    "test_text_feat = merge_parts(pm.result, \"get_text_feat_test\", 14) # 执行结果为 函数名_给定标签\n",
    "\n",
    "# # 2min43ss\n",
    "logger.info('输出文本特征...')\n",
    "train_text_feat.to_hdf(train_text_feat_path, '12345', mode='w')\n",
    "test_text_feat.to_hdf(test_text_feat_path, '12345', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "311E6C1C435847058F61BC563079D252",
    "mdEditEnable": false
   },
   "source": [
    "### 统计特征（跑完后须要重启kernel运行特征工程前所有cell再提取下一个特征）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9123A4F6AE9848838FF002DE4B6901AB"
   },
   "outputs": [],
   "source": [
    "def get_cnt_feat(data):\n",
    "    '''\n",
    "        提取统计特征\n",
    "            query/title word num\n",
    "            query/title 交集的单词数\n",
    "            query/title 除去title的单词数/除去query的单词数\n",
    "            query/title 出现次数\n",
    "    '''\n",
    "    c_f = data[['query_id', 'query','query_title_id', 'title']]\n",
    "    \n",
    "    logger.info('提取query长度特征...')\n",
    "    c_f['query_num'] = [len(i.split(' ')) for i in c_f['query']]\n",
    "    logger.info('提取title长度特征...')\n",
    "    c_f['title_num'] = [len(i.split(' ')) for i in c_f['title']]\n",
    "    \n",
    "    logger.info('提取query/title出现次数...')\n",
    "    query_cnt = c_f['query'].value_counts().reset_index(name='query_cnt').rename(columns={'index':'query'})\n",
    "    c_f = c_f.merge(query_cnt, how='left', on='query')\n",
    "    \n",
    "    title_cnt = c_f['title'].value_counts().reset_index(name='title_cnt').rename(columns={'index':'title'})\n",
    "    c_f = c_f.merge(title_cnt, how='left', on='title')\n",
    "    return c_f.drop(['query', 'title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2042BC9CEDDB421BAC4E18D815D39B5D"
   },
   "outputs": [],
   "source": [
    "logger.info('提取训练集统计特征...')\n",
    "train_cnt_feat = get_cnt_feat(train)\n",
    "train_cnt_feat.to_hdf('/home/kesci/work/lmm/feature/train_cnt_feat.h5', '12345', mode='w')\n",
    "logger.info('提取测试集统计特征...')\n",
    "test_cnt_feat = get_cnt_feat(test)\n",
    "test_cnt_feat.to_hdf('/home/kesci/work/lmm/feature/test_cnt_feat.h5', '12345', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5B6700B51ADC450C8E375835E1076D88",
    "mdEditEnable": false
   },
   "source": [
    "### 语义特征（跑完后须要重启kernel运行特征工程前所有cell再提取下一个特征）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37F705D5E3DB45D287DAA04F0AC4B38E"
   },
   "outputs": [],
   "source": [
    "def jaccard(seq1, seq2):\n",
    "\t\"\"\"\n",
    "\tThe return value is a float between 0 and 1, where 0 means equal, and 1 totally different.\n",
    "\t\"\"\"\n",
    "\tset1, set2 = set(seq1), set(seq2)\n",
    "\treturn 1 - len(set1 & set2) / float(len(set1 | set2))\n",
    "\n",
    "\n",
    "def sorensen(seq1, seq2):\n",
    "\t\"\"\"\n",
    "\tThe return value is a float between 0 and 1, where 0 means equal, and 1 totally different.\n",
    "\t\"\"\"\n",
    "\tset1, set2 = set(seq1), set(seq2)\n",
    "\treturn 1 - (2 * len(set1 & set2) / float(len(set1) + len(set2)))\n",
    "\n",
    "\n",
    "def levenshtein(seq1, seq2, max_dist=-1):\n",
    "    \"\"\"\n",
    "    seq1, seq2:空格分隔的句子\n",
    "    \"\"\"\n",
    "    if seq1 == seq2:\n",
    "        return 0\n",
    "    seq1, seq2 = seq1.split(), seq2.split()\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "    if max_dist >= 0 and abs(len1 - len2) > max_dist:\n",
    "        return -1\n",
    "    if len1 == 0:\n",
    "        return len2\n",
    "    if len2 == 0:\n",
    "        return len1\n",
    "    if len1 < len2:\n",
    "        len1, len2 = len2, len1\n",
    "        seq1, seq2 = seq2, seq1\n",
    "    \n",
    "    column = array('L', range(len2 + 1))\n",
    "    \n",
    "    for x in range(1, len1 + 1):\n",
    "        column[0] = x\n",
    "        last = x - 1\n",
    "        for y in range(1, len2 + 1):\n",
    "            old = column[y]\n",
    "            cost = int(seq1[x - 1] != seq2[y - 1])\n",
    "            column[y] = min(column[y] + 1, column[y - 1] + 1, last + cost)\n",
    "            last = old\n",
    "        if max_dist >= 0 and min(column) > max_dist:\n",
    "            return -1\n",
    "    \n",
    "    if max_dist >= 0 and column[len2] > max_dist:\n",
    "        # stay consistent, even if we have the exact distance\n",
    "        return -1\n",
    "    return column[len2]\n",
    "\n",
    "\n",
    "def cos_dist(w1, w2):\n",
    "    dist1=float(np.dot(w1, w2)/(np.linalg.norm(w1)*np.linalg.norm(w2)))\n",
    "    return dist1\n",
    "\n",
    "\n",
    "def get_min_max_scaler(x):\n",
    "    return pd.Series((x-x.min()) / (x.max() - x.min()))\n",
    "\n",
    "\n",
    "def cos_feat(query, title):\n",
    "    cos_q_t = []\n",
    "    count = 0\n",
    "    wv1 = []\n",
    "    wv2 = []\n",
    "    for q, t in zip(query, title):\n",
    "        count += 1\n",
    "        key_word = list(set(q + t))\n",
    "        word_vector1 = np.zeros(len(key_word))\n",
    "        word_vector2 = np.zeros(len(key_word))\n",
    "        \n",
    "        for i in range(len(key_word)):\n",
    "            for j in range(len(q)):\n",
    "                if key_word[i] == q[j]:\n",
    "                    word_vector1[i] += 1\n",
    "            wv1.append(word_vector1)\n",
    "            for k in range(len(t)):\n",
    "                if key_word[i] == t[k]:\n",
    "                    word_vector2[i] += 1\n",
    "            wv2.append(word_vector2)\n",
    "            \n",
    "        cos_q_t.append(cos_dist(word_vector1, word_vector2))\n",
    "    \n",
    "    return cos_q_t\n",
    "\n",
    "# start\n",
    "pm = ParallelMachine() # 实例化pm对象 \n",
    "\n",
    "def areas(total, pieces):\n",
    "    return (((total * i)// pieces, (total * (i + 1)) // pieces) for i in range(pieces))\n",
    "    \n",
    "train_mapping = {f\"train_{i}\": train.iloc[area[0]:area[1], :] for i, area in enumerate(areas(train.shape[0], 14))}\n",
    "test_mapping = {f\"test_{i}\": test.iloc[area[0]:area[1], :] for i, area in enumerate(areas(test.shape[0], 14))}\n",
    "\n",
    "job_mapping = {**train_mapping, **test_mapping}\n",
    "# end\n",
    "\n",
    "@pm.job(job_mapping)\n",
    "def get_semantic_feat(data):\n",
    "    '''\n",
    "        相似度\n",
    "    '''\n",
    "    s_f = data[['query_id', 'query', 'query_title_id', 'title']]\n",
    "    logger.info('get jaccard distance...')\n",
    "    s_f['q_t_jaccard'] = [jaccard(i, j) for i, j in zip(s_f['query'], s_f['title'])]\n",
    "    \n",
    "    logger.info('get sorensen distance...')\n",
    "    s_f['q_t_sorensen'] = [sorensen(i, j) for i, j in zip(s_f['query'], s_f['title'])]\n",
    "    \n",
    "    logger.info('get levenshtein distance...')\n",
    "    s_f['q_t_levenshtein'] = [levenshtein(i, j) for i, j in zip(s_f['query'], s_f['title'])]\n",
    "\n",
    "    logger.info('计算余弦相似度...')    \n",
    "    s_f['cos_q_t'] = cos_feat(data['query'], data['title'])\n",
    "\n",
    "    return s_f.drop(['query', 'title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDD342B55EC840CF90C9058284837993"
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "pm.mutilprocess(worker=10, timeout=None) # 多进程\n",
    "def merge_parts(resultset, prefix, r):\n",
    "    return reduce(lambda d1, d2: d1.append(d2), (resultset[f\"{prefix}_{i}\"] for i in range(r)))\n",
    "    \n",
    "logger.info('提取训练集语义...') \n",
    "train_semantic_feat = merge_parts(pm.result, \"get_semantic_feat_train\", 14) # 执行结果为 函数名_给定标签 \n",
    "\n",
    "# 19min58s\n",
    "logger.info('提取测试集语义...') \n",
    "test_semantic_feat = merge_parts(pm.result, \"get_semantic_feat_test\", 14) # 执行结果为 函数名_给定标签\n",
    "\n",
    "train_semantic_feat.to_hdf('/home/kesci/work/lmm/feature/train_semantic_feat.h5', '12345', mode='w')\n",
    "test_semantic_feat.to_hdf('/home/kesci/work/lmm/feature/test_semantic_feat.h5', '12345', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4449C759FB0C43BB96C2F43863D81C27",
    "mdEditEnable": false
   },
   "source": [
    "### 交集特征（跑完后须要重启kernel运行特征工程前所有cell再提取下一个特征）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2A01EBD139D4C579DCE7A31AE8977DA"
   },
   "outputs": [],
   "source": [
    "# start\n",
    "pm = ParallelMachine() # 实例化pm对象 \n",
    "\n",
    "def areas(total, pieces):\n",
    "    return (((total * i)// pieces, (total * (i + 1)) // pieces) for i in range(pieces))\n",
    "    \n",
    "train_mapping = {f\"train_{i}\": train.iloc[area[0]:area[1], :] for i, area in enumerate(areas(train.shape[0], 14))}\n",
    "test_mapping = {f\"test_{i}\": test.iloc[area[0]:area[1], :] for i, area in enumerate(areas(test.shape[0], 14))}\n",
    "\n",
    "job_mapping = {**train_mapping, **test_mapping}\n",
    "# end\n",
    "\n",
    "@pm.job(job_mapping)\n",
    "def intersection_words_feat(data):\n",
    "    sum_q_t_intersection = []\n",
    "    mean_q_t_intersection = []\n",
    "    var_q_t_intersection = []\n",
    "    for i, j in zip(data['query'], data['title']):\n",
    "        q_t = []\n",
    "        q = i.split(' ')\n",
    "        t = j.split(' ')\n",
    "        for k in q:\n",
    "            q_t.append(t.count(k))\n",
    "        sum_q_t_intersection.append(sum(q_t))\n",
    "        mean_q_t_intersection.append(sum(q_t)/len(q_t))\n",
    "        var_q_t_intersection.append(np.array(q_t).var())\n",
    "    i_w_f = data[['query_id', 'query_title_id']]\n",
    "    i_w_f['q_t_intersection_sum_cnt'] = sum_q_t_intersection\n",
    "    i_w_f['q_t_intersection_mean_cnt'] = mean_q_t_intersection\n",
    "    i_w_f['q_t_intersection_var_cnt'] = var_q_t_intersection\n",
    "    return i_w_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64F27CA45C9F4242B0AC4A477F7E9645"
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "pm.mutilprocess(worker=10, timeout=None) # 多进程\n",
    "def merge_parts(resultset, prefix, r):\n",
    "    return reduce(lambda d1, d2: d1.append(d2), (resultset[f\"{prefix}_{i}\"] for i in range(r)))\n",
    "logger.info('交集出现次数')\n",
    "train_q_t_intersection = merge_parts(pm.result, \"intersection_words_feat_train\", 14) # 执行结果为 函数名_给定标签\n",
    "\n",
    "logger.info('交集出现次数')\n",
    "test_q_t_intersection = merge_parts(pm.result, \"intersection_words_feat_test\", 14) # 执行结果为 函数名_给定标签\n",
    "\n",
    "train_q_t_intersection.to_hdf('/home/kesci/work/lmm/feature/train_q_t_intersection.h5', '12345', mode='w')\n",
    "test_q_t_intersection.to_hdf('/home/kesci/work/lmm/feature/test_q_t_intersection.h5', '12345', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFDE851F1E4A4ADB82184E48357F7422",
    "mdEditEnable": false
   },
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7C2BF0939724DA2A90E14D679D25A21"
   },
   "outputs": [],
   "source": [
    "def lgb_predict(train_f, train_label, val_f, val_label):\n",
    "    lgb_train = lgb.Dataset(train_f, train_label)\n",
    "    lgb_eval = lgb.Dataset(val_f, val_label)\n",
    "    # specify your configurations as a dict\n",
    "\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'boost_from_average': False,\n",
    "        'metric': 'auc',\n",
    "        'num_leaves': 30,\n",
    "        'min_data_in_leaf':50,\n",
    "        'max_depth':5,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'n_jobs':4\n",
    "    }\n",
    "    print('Start training...')\n",
    "    # train\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=1000,\n",
    "                    valid_sets=lgb_eval)\n",
    "    print('Save model...')\n",
    "    # save model to file\n",
    "    gbm.save_model('/home/kesci/work/lmm/model/lgb_model.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36B6341D31424A228EF49E8D903EAB1F",
    "mdEditEnable": false
   },
   "source": [
    "### 重启kernel运行特征工程前所有cell再运行以下cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46D853BA470046B282B0FF972333E9BE"
   },
   "outputs": [],
   "source": [
    "train_text_feat = pd.read_hdf('/home/kesci/work/lmm/feature/train_text_feat.csv', '12345')\n",
    "train_semantic_feat = pd.read_hdf('/home/kesci/work/lmm/feature/train_semantic_feat.h5', '12345')\n",
    "train_cnt_feat = pd.read_hdf('/home/kesci/work/lmm/feature/train_cnt_feat.h5', '12345')\n",
    "train_q_t_intersection = pd.read_hdf('/home/kesci/work/lmm/feature/train_q_t_intersection.h5', '12345')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A643898FE444253835086E3C847A4E8"
   },
   "outputs": [],
   "source": [
    "logger.info('merge 文本特征...')\n",
    "train_feat = train[['query_id', 'query_title_id', 'label']].merge(train_text_feat, on=['query_id', 'query_title_id'], how='left')\n",
    "logger.info('merge 统计特征...')\n",
    "train_feat = train_feat.merge(train_cnt_feat, on=['query_id', 'query_title_id'], how='left')\n",
    "logger.info('merge 语义特征...')\n",
    "train_feat = train_feat.merge(train_semantic_feat, on=['query_id', 'query_title_id'], how='left')\n",
    "logger.info('merge 交集单词出现次数特征...')\n",
    "train_feat = train_feat.merge(train_q_t_intersection, on=['query_id', 'query_title_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6F0063BE89F6475B83A71732578B5882"
   },
   "outputs": [],
   "source": [
    "del train_text_feat\n",
    "del train_semantic_feat\n",
    "del train_cnt_feat\n",
    "del train_q_t_intersection\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0557D9F55D648CC84792290F156713D"
   },
   "outputs": [],
   "source": [
    "train_f = train_feat[-48000000:].drop(['query_id', 'query_title_id', 'label'], axis=1)\n",
    "val_f = train_feat[:2000000].drop(['query_id', 'query_title_id', 'label'], axis=1)\n",
    "\n",
    "train_label = train_feat['label'][-48000000:]\n",
    "val_label = train_feat['label'][:2000000]\n",
    "\n",
    "lgb_predict(train_f, train_label, val_f, val_label, test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BCF67FA1DE043F29F3570A1E2A788E8",
    "mdEditEnable": false
   },
   "source": [
    "### 重启kernel运行特征工程前所有cell再运行以下cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0D0B62D1ACFE4CEBB673B8D54A303B20"
   },
   "outputs": [],
   "source": [
    "test_text_feat = pd.read_hdf('/home/kesci/work/lmm/feature/test_text_feat.csv', '12345')\n",
    "test_semantic_feat = pd.read_hdf('/home/kesci/work/lmm/feature/test_semantic_feat.h5', '12345')\n",
    "test_cnt_feat = pd.read_hdf('/home/kesci/work/lmm/feature/test_cnt_feat.h5', '12345')\n",
    "test_q_t_intersection = pd.read_hdf('/home/kesci/work/lmm/feature/test_q_t_intersection.h5', '12345')\n",
    "\n",
    "logger.info('merge 文本特征...')\n",
    "test_feat = test[['query_id', 'query_title_id']].merge(test_text_feat, on=['query_id', 'query_title_id'], how='left')\n",
    "logger.info('merge 统计特征...')\n",
    "test_feat = test_feat.merge(test_cnt_feat, on=['query_id', 'query_title_id'], how='left')\n",
    "logger.info('merge 语义特征...')\n",
    "test_feat = test_feat.merge(test_semantic_feat, on=['query_id', 'query_title_id'], how='left')\n",
    "logger.info('merge 交集单词出现次数特征...')\n",
    "test_feat = test_feat.merge(test_q_t_intersection, on=['query_id', 'query_title_id'], how='left')\n",
    "\n",
    "test_f = test_feat.drop(['query_id', 'query_title_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54FA3882F3B54A9285BAC0855D0A684B"
   },
   "outputs": [],
   "source": [
    "del test_text_feat\n",
    "del test_semantic_feat\n",
    "del test_cnt_feat\n",
    "del test_q_t_intersection\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1ACDBE45D4B4CE989ED435778831D3E"
   },
   "outputs": [],
   "source": [
    "gbm = lgb.Booster(model_file='/home/kesci/work/lmm/model/lgb_model.txt')\n",
    "pred = gbm.predict(test_f, num_iteration=gbm.best_iteration)\n",
    "feat_importances = pd.DataFrame({\n",
    "    'column': test_f.columns,\n",
    "    'importance': gbm.feature_importance(),\n",
    "}).sort_values(by='importance')\n",
    "plt.figure(figsize=(12,6))\n",
    "lgb.plot_importance(gbm, max_num_features=30)\n",
    "plt.title(\"Featurertances\")\n",
    "plt.show()\n",
    "pre = list(pred)\n",
    "prob = test[['query_id', 'query_title_id']]\n",
    "prob['label'] = pre\n",
    "prob.to_csv('/home/kesci/work/lmm/output/final_test.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1F7B2F1856BC4A4DA2A948C9285D9092",
    "mdEditEnable": false
   },
   "source": [
    "# 十、融合方案(线性加权)，最终成绩a榜：0.619+，b榜：0.645+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DC868DC3CC364F748D43105E95BA1EA2"
   },
   "outputs": [],
   "source": [
    "# 显示cell运行时长\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "420C9FA7F3D44137933E7CAD1B11F50A"
   },
   "outputs": [],
   "source": [
    "# 最终预测\n",
    "r1 = pd.read_csv(\"stage2_improved_textcnn_feat_submit_v0.csv\", names=['query_id','query_title_id', 'pred'])\n",
    "r2 = pd.read_csv(\"stage2_LSTM_FastText_feat_submit_Epoch1.csv\", names=['query_id','query_title_id', 'pred'])\n",
    "r3 = pd.read_csv(\"lgb_hao_final.csv\", names=['query_id','query_title_id', 'pred'])\n",
    "r4 = pd.read_csv(\"stage2_lgb_submit_v0.csv\", names=['query_id','query_title_id', 'pred'])\n",
    "r5 = pd.read_csv(\"lmm/output/final_test.csv\", names=['query_id','query_title_id', 'pred'])\n",
    "new = pd.read_csv(\"stage2_LSTM_CNN_feat_submit_Epoch3.csv\", names=['query_id','query_title_id', 'pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CE002CD052AC4F0094464DB0DC8E31CF"
   },
   "outputs": [],
   "source": [
    "# 两个603 nn\n",
    "res = r1.copy()\n",
    "res.pred = r1.pred * 0.5 + r2.pred * 0.5\n",
    "# 三个lgb\n",
    "res2 = r1.copy()\n",
    "res2.pred = r3.pred * 0.4 + r4.pred * 0.4 + + r5.pred * 0.2\n",
    "# nn和lgb\n",
    "fuse1 = r1.copy()\n",
    "fuse1.pred = res.pred * 0.6 + res2.pred * 0.4\n",
    "# 融610 nn\n",
    "fuse5 = fuse1.copy()\n",
    "fuse5.pred = fuse1.pred * 0.6 + new.pred * 0.4\n",
    "fuse5.pred = fuse5.pred.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7A356FC446E488982D87A0318A11290"
   },
   "outputs": [],
   "source": [
    "fuse5.to_csv(\"/home/kesci/work/final1.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "503D56E2B8DE4EFF9244DA3A9263DEBA"
   },
   "outputs": [],
   "source": [
    "!https_proxy=\"http://klab-external-proxy\" ./kesci_submit -file final1.csv -token 385da1c2ad16422b -mode archive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
